WE can save the credentials wioth the help of the 

1. By putting out credentials directly in TF (Not recommneded)
2. By adding the profile in the aws credentials (used widely)
3. Adding details via Env variables (Mostly used in Pipelines Framework)

    export AWS_ACCESS_KEY_ID = "your key"
    export AWS_SECRET_ACCESS_KEY = "your secret access key"
    Run terraform plan .. It must work

4. By installing TF on instances we can assign a role to the ec2 instance so that we can use it more securely and then access directly (Widely used; Best practice; more secured)

############    Day 6  USing STS  ################### 

STS [Secure Token Service] in AWS ; It is used to generate a token for the short time. For Min time we can set ourselves; Max for 12 hours.
It gives access key; secret key and Session Token generation is provided with the expiration date


 Commands followed during the creation of Secure Token Service execution is as follows
 
1. create a user "stsuser" with no permissions --> no policy attached --> add it to the aws credentials file with the same profile name 	 
  aws configure --profile stsuser
  cat ~/.aws/credentials 
  aws s3 ls (Fail)
  
2. Create a role --> For aws account --> Admin Access(Highest Privileage) --> name as "stsassumerole"
  aws s3 ls (Fail)

3. Need to integrate the created User with the role

a. On user End --> Permission --> Need to add a inline policy --> Using template for using the IAM ROle in the AWS CLI --> Change the arn with the arn of the role we need to assume. --> name it as "stsassumerolepolicy"

b. On the Role End --> It needs to trust the user so refer to trust relationships --> Exactly the same as in the https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-role.html

aws s3 ls (Fails) --> Its still assuming that we are executing it with the current user but we need to implement it via the role(that we recently created) --> Follow the command  

a. aws sts assume-role --role-arn arn:aws:iam::972348856143:role/stsassumerole --role-session-name accesss3
  
b. aws sts get-caller-identity --> It shows out the current user in my case gives terraform user
  
To configure which profile to be used 
  
Just edit out the file using -->code ~/.aws/config
Add a profile with the fields role_arn (arn of the role), source_profile(the credential for which we want to assume the role)
  
[profile sts]
role_arn = <arn of the role>
source_profile = <credential for which we want to assume role = stsuser in my case>  
  
For source profile credentails we can verify it via the --> cat ~/.aws/credentials
  
  
then if everything is configured

aws s3 ls --profile sts --> Lists out the s3 bucket lists

This should work as it was working fine for some time now

Try creating any new user--> add credentials in the aws credentails folder --> Do not give any permission to the user then ty accessing suing the command 

aws s3 ls --profile new_user (It will not work)

#########################  END Day 6 ####################################### 

########################## Day 7 Creating VPC via Terraform [7 June 2023] ###############################################

For multiple account the role is configured and centralized account where user is configured then how to configure it suing the Terraform

just add the assume_role block in the provider.tf file

assume_role {
  role_arn = "your_arn"
  session_name = "your seesion name" [It is similiar to as that of the [aws sts --role-arn <arn of the role to be assumed> --role-session-name <your session name>] Also remember the session maxes out at 12 hrs so need to create a new token after that]
}

How to actually create a VPC using the TF ?

1.In VPC there are lots of fields known as Arguments which are needed to fill to host VPC {Some mandatory or Optional}

2. In tf planning if we do not want hassle to cross check then give terraform plan --auto-approve

3. For generating VPC we need the bare minimum as [cidr_block]  OR [ipv4_ipam_pool_id] (Which inturn requires the ipv4_netmask_length to calulcate the cidr range)

While creating a resource block 

resource "aws_vpc" "main" {}

aws_vpc -- represents what type of resource is expected by the provider to create.
main -- represents the name of that resource but standards should be followed like giving it names like this, main, my_vpc, main_vpc

To create vpc we need to provide the cidr ranges which can be easily get by --> search  private ip ranges and based on the requirement add a class range

to save a plan in teraform just add the --> terraform plan -out sample_name.out (File will be generated with such name)

to execute that just use --> terraform apply "sample_name.out" (It would create the vpc resource in aws console we caqn verify)

########################### End Day 7 [7 June 2023] #########################################

########################### Day 8 ##################################################
######################################### END Day 8 ########################################

###########################  Day 9 Creation of Aws 3-tier architecture by using for each, Count in terraform ,  some basic git commands cm [8 June 2023] #########################################

#1st Iteration

# resource "aws_subnet" "this" {
#   # for_each   = toset(["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", " 192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"])
#   for_each   = { "1": "192.168.0.0/27", "2":"192.168.0.32/27", "3": "192.168.0.64/27", "4": "192.168.0.96/27", "5": "192.168.0.128/27", "6": "192.168.0.160/27"}
#   vpc_id     = aws_vpc.this.id
#   cidr_block = each.value

#   tags = {
#     Name = "Main_${each.key}"
#   }
# }


# resource "aws_subnet" "this" {
#   # count  = var.no_of_subnets
#   count = length(var.cidr_subnet)

#   #length is used to find the length of list
#   vpc_id = aws_vpc.this.id
#   #element helps us to traverse through the list one element at a time
#   # In terraform a list can have same type of dataype; as in python
#   cidr_block = element(var.cidr_subnet, count.index)
#   tags = {
#     Name = "subnet-${count.index}"
#   }
# }

variable "no_of_subnets" {
  type        = number
  description = "Number of subnets to be created"
  default     = 6 #If variable is not passed then waht value needed to be considered
}

variable "cidr_subnet" {
  type        = list(string)
  description = "List of CIDR range in Subnets"
  default     = ["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", "192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"]
}

#terraform console used to checkout the functions functioanlity of TF in the console itself.

# Now we need to create public and private subnets as per our discussions

# resource "aws_subnet" "this" {
#   # for_each   = toset(["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", " 192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"])
#   for_each   = { "public_subnet_1" : "192.168.0.0/27", "public_subnet_2" : "192.168.0.32/27", "private_subnet_1" : "192.168.0.64/27", "private_subnet_2" : "192.168.0.96/27", "private_subnet_3" : "192.168.0.128/27", "6" : "192.168.0.160/27" }
#   vpc_id     = aws_vpc.this.id
#   cidr_block = each.value
#   availability_zone = 

#   tags = {
#     Name = "Main_${each.key}"
#   }
# }

#for_each is widely usedas compared to the count

#But we need to consider the point of different AVALABILTY ZONES
# As per our requiremnts we need to have the 2 Avalibilty zones

#Search for it in the arguments refrences

#pass avalibility zones but it takse string as input and we need to provide the list here

#we need to pass 1 public and 2 private in each AZ i.e. for 6 subnets we will consider here 2 AZ i.e. us-east-1a, us-east-1b

# 1 way is by creating 1 subnet for AZ 1 and another subnet for AZ2

resource "aws_subnet" "subnet_az1" {
  # for_each   = toset(["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", " 192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"])
  for_each          = { "public_subnet_1_az1" : "192.168.0.0/27", "private_subnet_1_az1" : "192.168.0.32/27", "private_subnet_2_az1" : "192.168.0.64/27" }
  vpc_id            = aws_vpc.this.id
  cidr_block        = each.value
  availability_zone = "us-east-1a"

  tags = {
    Name = "Subnet_${each.key}"
  }
}

resource "aws_subnet" "subnet_az2" {
  # for_each   = toset(["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", " 192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"])
  for_each          = { "public_subnet_1_az2" : "192.168.0.96/27", "private_subnet_1_az2" : "192.168.0.128/27", "priavte_subnet2_az2" : "192.168.0.160/27" }
  vpc_id            = aws_vpc.this.id
  cidr_block        = each.value
  availability_zone = "us-east-1b"

  tags = {
    Name = "Subnet_${each.key}"
  }
}

#But the above process goes through the lot of code repetation


########################### End Day 9 [8 June 2023] #########################################

########################## Day 10 [] #######################################################

Disusions about the JIRA board

To create a branch using the story provided we should use the following like [orgname-jira.story.no-branchname] this is the standard followed throughout

create variable.tf
variable "cidr_for_vpc" {
    description = "the cidr range for VPC"
    type = string
}

#If we want to input the value from the user then we do not need to add the "default"


enable_dns_support - (Optional) A boolean flag to enable/disable DNS support in the VPC. Defaults true.

enable_dns_hostnames - (Optional) A boolean flag to enable/disable DNS hostnames in the VPC. Defaults false.

What if we need ourr resources into multiple avalilale zones then we need to add a Data Source to feed that how many number of avalibilty zones are there by creating data.tf file

Data source is special kind of block like resource block in TF which actually retrieve the information as per the defination

eg how many AZ available in north virginia

i.e. basically it doesnt create any resource but it actually helps us out in the info about anything available in resources like aws, azure, gcp



eg in this vpc we nned to create eks cluster but its in another foleder with the help of datasource we can get the id of the vpc and pass it to the eks cluster

add data.tf file and search for the datasource in terrafrom get this template from the below
From here we will get the final 

data "aws_availability_zones" "this" {
  all_availability_zones = true
  filter {
    name   = "opt-in-status"
    values = ["opt-in-not-required"]
  }
}

Creating 2 blocks for the private subnet and the public subnet

remove the existing code

resource "aws_subnet" "subnet_az1" {
  # for_each   = toset(["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", " 192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"])
  for_each          = { "public_subnet_1_az1" : "192.168.0.0/27", "private_subnet_1_az1" : "192.168.0.32/27", "private_subnet_2_az1" : "192.168.0.64/27" }
  vpc_id            = aws_vpc.this.id
  cidr_block        = each.value
  availability_zone = "us-east-1a"

  tags = {
    Name = "Subnet_${each.key}"
  }
}

resource "aws_subnet" "subnet_az2" {
  for_each          = { "public_subnet_1_az2" : "192.168.0.96/27", "private_subnet_1_az2" : "192.168.0.128/27", "priavte_subnet2_az2" : "192.168.0.160/27" }
  vpc_id            = aws_vpc.this.id
  cidr_block        = each.value
  availability_zone = "us-east-1b"

  tags = {
    Name = "Subnet_${each.key}"
  }
}

Suppose we need to create the private subnets fopr all tha availibility zones then add

for_each = { for index, az_name in data.aws_availability_zones.this.names : index => az_name }

for the for_each we are passing out the map datatype as it supports the map and set datatype
index - gives out the index of the subnets and 
az_name -- will give us out the avalibility zones 

data.aws_availability_zones.this -- It will let us know all the availibility zones in the particular region region
eg for us-east gives out 6 AZ
for mumbai fives out 3 AZ

now to get the xact name if the az we could use the attribute reference like "name" 
 data.aws_availability_zones.this.name

Now by this it will only fetch if for the single subnet but we want it for multiple subnets 

so here we requie for loops and In Terraform only for loop is supported not the while loop

for_each = { for index, az_name in data.aws_availability_zones.this.names : index => az_name } // it will give us ourt the o/p inform of map datatype

For map datatype it is a set value os requires in form key : value pairs

so we should add for the for loop as index => az_name

in 1st iteration it will act like for az_name in [us-east1a, us-east-1b...]
and it will iterate throughout the list which we get from the data source

index represents the index of the subnet 
az_name represents the names of the availability_zone


What if we do not want all the avalibilty zones but only some of them like us-east-1a, us-east-1b

Then here wecan follow a modular approach like using the "locals"

using locals we can asign then names to an expression so that we could use names multiple times within a module instead of repeating the expression

NOTE:  Local values are created by a locals block (plural), but you reference them as attributes on an object named local (singular). Make sure to leave off the "s" when referencing a local value!



locals {
  service_name = "forum"
  owner        = "Community Team"
}

and to retrieve vvalues form it 

resource "aws_instance" "example" {
  # ...

  tags = local.common_tags
}

we can use it in our case as 

data "aws_availability_zones" "this" {
}

locals {
  sorted_availability_zones   = sort(data.aws_availability_zones.all.names)
  selected_availability_zones = toset([
    local.sorted_availability_zones[0],
    local.sorted_availability_zones[1],
  ])
}

resource "aws_subnet" "public_subnet" {
  for_each = local.selected_availability_zones

  vpc_id = aws_vpc.this.id
  availability_zone = each.value
}

resource "aws_subnet" "private" {
  for_each = local.selected_availability_zones

  vpc_id = aws_vpc.this.id
  availability_zone = each.value
}

But this would onlyu work if we want the consecutive AZ and from the start


2nd Approach could be as follows like 

data "aws_region" "this" {}

locals {
 az1 = "${data.aws_region.this.name}a"
 az2 = "${data.aws_region.this.name}b"
}


The cidrsubnet(prefix, newbits, netnum) Syntax

prefix: it gives the cidr_range of vpc
newbits: how many subnets we need to create let us say 6 i.e. it can be satisfied with the help of 2^3 = 8 so 3 will act as the newbits
netnum(Important): 

eg 10.0.0.0/24 for creating 6 subnets --> 2^3 ie 3 will be new bits
3 bits we will be giving for subnet masking
10.0.0.0/24 in binary representation

8bits + 8bits + 8bits + 8bits = 32 bits 
24 bits already assigned to network
8 bits remains for the host -- out of which 3 we are adding to the host (As a part of adding the subnet)

Only 5 bits are available to the host now

So now 3 bits can be raging from 000 to 111
start from rightmost to the left
2^0 * 1 = 1 (maximum value it can have at that unit position)
2^1 * 1 = 2 (maximum value it can have at that tens position)
2^2 * 1 = 4 (maximum value it can have at that hundredth position)

Adding it up brings out 1 + 2 + 4 = 7(Max value it can obtain) 

7 acts as netnum here


git status --short // gives status befor staging

git log --oneline // gets logs in single line
########################## End Day 10 [] #######################################################

########################## Day 11 - cidrsubnet function in TF #################################

cidrsubnet(prefix, newbits, netnum)

calculating the cidr subnet values
prefix is nothing but the vpc_cidr range so we can obtain it like var.cidr_for_vpc (From the variables.tf file) OR we could also use the aws_vpc.this.cidr_block (valid way)

now we need to form a logic to get the newbits(how many subnets we need to create let us say 6 i.e. it can be satisfied with the help of 2^3 = 8 ;So 3 will act as the newbits)
logic : we can do that using length function of TF --> length(data.aws_availability_zones.this.names)

For the newbits => If subnets 3 ; It is satisfied by 2^2 = 4 ; 2 will be the newbits
                   If subnets 6 ; It will be satified by 2^3= 8 ; 3 will be the newbits


length(data.aws_availability_zones.this.names) > 3 ? 4 : 2 (USing ternary operator)


eg if cidr= 10.0.0.0/24
for 3 subnets => 2^2 = 4 staisfies; 2 newbits

cidr will now be 10.0.0.0/26

TASK: But we will not know the how many avaklibility zones would be there; but we need each AZ should contain a private subnet

NOTE: AZ will not be greater than 8 for now(Max to max in aws till now)

if suppose there we have 6 AZ then in that case; as per our condition we require each AZ should contain each private subnet so we reuire 6 subnets

length(data.aws_availability_zones.this.names) gives us 6

Example: for 6 subnets how much the new bits are required ? Calculate

6 => 2^ 3 = 8 ; 3 new bits will satisy our needs

Earlier: length(data.aws_availability_zones.this.names) > 3 ? 4(Indicates true part) : 2 (Indicates false part)
Changed: length(data.aws_availability_zones.this.names) > 4 ? 3(Required newbits) : 2
         
Decoding Logic   In our case for 6 subnets =>  6 > 4  its true so it will trigger true part i.e. 3
As per our ternary condition we need to make some changes here like
NOTE: It would work for the requirement form 5/6/7/8 subnets in total


FOr netnum calculation

eg 10.0.0.0/24 for creating 6 subnets --> 2^3 ie 3 will be new bits
3 bits we will be giving for subnet masking
10.0.0.0/24 in binary representation

8bits + 8bits + 8bits + 8bits = 32 bits 
24 bits already assigned to network
8 bits remains for the host -- out of which 3 we are adding to the host (As a part of adding the subnet)

Only 5 bits are available to the host now

So now 3 bits can be raging from 000 to 111
start from rightmost to the left
2^0 * 1 = 1 (maximum value it can have at that unit position)
2^1 * 1 = 2 (maximum value it can have at that tens position)
2^2 * 1 = 4 (maximum value it can have at that hundredth position)

Adding it up brings out 1 + 2 + 4 = 7(Max value it can obtain) 

7 acts as netnum here [Ranges from 0 to 7]

so net num should move form 0 to 7 

We cannot us for loop as its already in use ; no foreach loop as its already in use;

But we can see as we arre calculating the map in the for_each
for_each   = { for index, az_name in data.aws_availability_zones.this.names : index => az_name }

as the for_each loop only supports map and set and here it is providing us with the map ; so we could make use of the key value pair in it like {"1": "us-east-1a", "2": "us-east-1b"...}
as we just wanted the number so we can just loop in the by using "each.key"

Similarly to find the avalibility zones we have can fetch it as ==> each.value

-----------
Scenario: So now in the case of the public subnet we need to change the IP Adresses as it would result into the Ip Address Conflict (as private and the public subnet should not contain the same IP)

here we are creating 6 subnets in total for public and private subnets

for 6 subnets new bits will be 3 (Calculated above)

Now our biggest challenge will be the asigning diffrnt ranges for public and private Subnets

length(data.aws_availability_zones.this.names) > 4 ? 3(Required newbits) : 2
Change1 : length(data.aws_availability_zones.this.names) > 3 ? 3(Required newbits) : 2 (It will fail when the subnets are less than 3 here as it would require 2 as newbits)
Change2 : length(data.aws_availability_zones.this.names) > 3 ? 4(Required newbits) : 3 ()
Change3: Now we need to find the netnum such taht the Ips do not coincide with each other

         length(data.aws_availability_zones.this.names) > 3 ? 4 : 3, each.key+length(data.aws_availability_zones.this.names)    

         Let us consider for the
         Public ==> 1-3 subnets should be there
         Private ==> 4-6 subnets should be there so by adding this length(data.aws_availability_zones.this.names) > 3 ? 4 : 3, each.key+length(data.aws_availability_zones.this.names) it would work as charm    

Or Change 4: simply we could just add it into the index of the private subnets so it will just iterate it from that number of index

        Initially index=0 , length(data.aws_availability_zones.this.names) > 3 ? 4 : 3, each.key+length(data.aws_availability_zones.this.names) let be 3
        index+(data.aws_availability_zones.this.names) > 3 ? 4 : 3, each.key+length(data.aws_availability_zones.this.names) = 0+3 =3

        next iteration ==>  1 + 3 = 4
        next iteration ==>  2 + 3 = 5
        next iteration ==>  3 + 3 = 6
        

########################## END Day 11 ########################################################

############################## Day 12 Add dynamic variables and Route Tables########################################################

Scenario: In our earlier case we have built 6 oublic and 6 private subnets but we want to limit it for each AZ 1 public and 1 private subnets

For this we can use function slice

slice -- Syntax: slice(list, start index, end index)

NOTE: It would only work on the list dataype and end index will not be considered in the list


adding the slice in the "private_subnet" for loop here we will get the follwing { for index, az_name in slice(data.aws_availability_zones.this.names, 0, 2) : index => az_name }
so by the steps above it will create 2 Avalibilty zones as per our condition we require same number of subnets as avalibilty zones so we get 2 subnets in the private
same piece of chaneges for the "Private subnet" also ==> 2 public subnet here also created (By above logic)



For 4 subnets(2 public and private) the new newbits will be as follows

2^2 = 4 satisfies ==> 2 will be the netnum then

so we need to change the condition of our ternary operator as follows

Earlier: length(data.aws_availability_zones.this.names) > 3 ? 4 : 3
Changed: length(data.aws_availability_zones.this.names) > 3 ? 4 : 3
          4 < 3 ? 2 : 3
so now it will traverse from the 0th index avalilbilty zone eg us-east-1a ; 1st index: will be us-east-1b; it will be there only  
Also the index+(data.aws_availability_zones.this.names) gives out some problem so we can move that length(data.aws_availability_zones.this.names) to each.key(As seen on day 11)

-------------------------
To passs the variables in here we can do it by many ways 

way1 : is by passing variables

terraform apply --var "cidr_for_vpc=10.0.0.0/24" --var "vpc_name=vpc_3tier"

terraform apply --auto-approve -var="cidr_for_vpc=10.0.0.0/24"

way2 : It is by adding the file like [network.tfvars] and add the variable here

cidr_for_vpc= "10.0.0.0/24"
vpc_name = "terraform_vpc"
NOte: we can also define(override) the existing ones which ahve also defined in their default fields
eg tenancy = dedicated

While destroying even if it takes the var argumnets it is of no use here in our case

terraform plan --auto-approve  -var-file = "network.tfvars"

if do not want to write the dediacated file name in -var-file then just change the name of the file to the following format

network.auto.tfvars ==> try to run using terraform apply then it would automatically consider your file and the variables in it will get passed

way3 : By adding the file terraform.tfvars (nod need to ad auto here as it has higher preceddence)

It acts a the default file for the terraform conisdering all thetfvars file.

During destroying aslo it doesnt take any input after apssing it the terraform.tfvars file

way4 : By adding the Environment variables for the variables

export TF_VAR_cidr_for_vpc = "10.0.0.0/24"
export TF_VAR_cidr_for_vpc="10.0.0.0/24"

NOTE: But once the session is over (TErminal shuts dowm) the ENV_VARIABLES get destroyed


Precedence order (last has the highest priority)

Environment variables
The terraform.tfvars file, if present.
The terraform.tfvars.json file, if present.
Any *.auto.tfvars or *.auto.tfvars.json files, processed in lexical order of their filenames.
Any -var and -var-file options on the command line, in the order they are provided. (This includes variables set by a Terraform Cloud workspace.)



Resource for Route Tables

We need to add 1 default route table. If we aare making change in the route table then all the previous routes will be scraped and only new ones would get considered

// priavte Route table 
resource "aws_default_route_table" "this" {
  default_route_table_id = aws_vpc.this.default_route_table_id

  # WE do not need the route for the local by default it will be added

  tags = {
    Name = "private_routetable_${var.vpc_name}"
  }
}

// Public Route Table we need to provide the internet gateway such taht the access should be allowed by everyone

Also for the public route table we need to add the internet_gateway as the ressource and should also check for the required dependencies of the internet gateway which is (the vpc_id in our case)

It should have target as internet gateway(that we need to create using the resources) and destination as 0.0.0.0/0 (Open for all i.e. it specifies all networks)


What is the 0.0.0.0/0 ?
The default route in Internet Protocol Version 4 (IPv4) is designated as the zero-address 0.0.0.0/0 in CIDR notation, often called the quad-zero route. 
The subnet mask is given as /0, which effectively specifies all networks, and is the shortest match possible.


resource "aws_route_table" "this" {
  vpc_id = aws_vpc.this.id

  # WE do not need the route for the local by default it will be added

  route {
    cidr_block = "0.0.0.0/0"
    # in public rt we require the internet gateway also
    gateway_id = aws_internet_gateway.this.id
  }

  tags = {
    Name = "public_routetable_${var.vpc_name}"
  }
}

#We need to add it and what arguments are required in it with the attribute refrences

resource "aws_internet_gateway" "this" {
  vpc_id = aws_vpc.this.id

  tags = {
    Name = "igw-${var.vpc_name}"
  }
}

making those changes we will get the error below
│ Error: creating Route in Route Table (rtb-062e33bad961ed4df) with destination (10.0.0.0/24): InvalidParameterValue: The destination CIDR block 10.0.0.0/24 is equal to or more specific than one of this VPC's CIDR blocks. This route can target only an interface or an instance.


Why is it facing error?

As we are providing the internet gateway we need to make it availble to access the internet i.e. it should be allowed for everyone


so in the public subnet change the field cidr_block we need to change the following as 0.0.0.0/0
now it should work as defined and with no error

 git log --oneline // gives the log of github
############################## END DAY 12 #####################################################

############################### DAY 13 #######################################################

WE have created the public and private route table ablove but we have not written the "subnet assosiated" with it; we need to add them 

When routes are created then we need to asosisate the subnets

gebnrally "this" is used when we have some unique resource eg only one vpc (so its better to understand)

If we are not multiple resourcesd then the resource name should be added as standard

search for aws_route_table ==>  we can get the folowing as 

resource "aws_route_table_association" "a" {
  subnet_id      = aws_subnet.foo.id
  route_table_id = aws_route_table.bar.id
}

Add for both the public and the private subnet
#route table assosiation for private  subnets
resource "aws_route_table_association" "private_subnet_assosiation" {
  subnet_id      = aws_subnet.private_subnet.id
  route_table_id = aws_default_route_table.this.id
}

#route table assosiation for public subnet
resource "aws_route_table_association" "public_subnet_assosiation" {
  subnet_id      = aws_subnet.public_subnet.id
  route_table_id = aws_route_table.this.id  # also check if we could add the id or not simply by searching it
}


It will throw out the error ? 

 71:   subnet_id      = aws_subnet.private_subnet.id
│ 
│ Because aws_subnet.private_subnet has "for_each" set, its
│ attributes must be accessed on specific instances.
│ 
│ For example, to correlate with indices of a referring
│ resource, use:
│     aws_subnet.private_subnet[each.key]
╵

How to resolve it then ?

As subnet_id      = aws_subnet.public_subnet.id it is a for_each and we cannot simply calculate it by this

As we do not have single id we have multiple public and private subnet so we need to make changes such taht we can make a refrence to all of the ids ?

We need t o use the "splat expressions" 

It is represented byu the help of the [*] ==> It can be directly used if instead of "for_each" it has "count"

In case of count it will iterate through all the ids like

resource "aws_route_table_association" "private_subnet_assosiation" {
  subnet_id      = aws_subnet.private_subnet[*].id
  route_table_id = aws_default_route_table.this.id
}

#route table assosiation for public subnet
resource "aws_route_table_association" "public_subnet_assosiation" {
  subnet_id      = aws_subnet.public_subnet[*].id
  route_table_id = aws_route_table.this.id  # also check if we could add the id or not simply by searching it
}

BUT as we have the for_each here so we can use the syntax below i.e.

[for o in var.list : o.interfaces[0].name]

resource "aws_route_table_association" "private_subnet_assosiation" {
  subnet_id      = [for each_subnet in aws_subnet.private_subnet: each_subnet.id]
  route_table_id = aws_default_route_table.this.id
}

#route table assosiation for public subnet
resource "aws_route_table_association" "public_subnet_assosiation" {
  subnet_id      = [for each_subnet in aws_subnet.public_subnet: each_subnet.id]
  route_table_id = aws_route_table.this.id
}


But still its not working as we are reciveing it in for expects a string

we can change the logic in it and alter it by adding a for_each loop

for_each = [for each_subnet in aws_subnet.private_subnet: each_subnet.id]

but for_each accepts either map or set
we convert it to the set using "toset"

for_each = toset[for each_subnet in aws_subnet.private_subnet: each_subnet.id]

 subnet_id      = each.key or each.value (Both are valid in the case of the set)


 try to terraform plan and terraform apply --auto-approve

 Check in the route table ==> subnet associations 

 For public route table we ae getting the public subnets attached to it
 For private route table we are getting the private subnet attached to it repectively

----------------
for now lets moveto output.tf file

The output.tf file helps in gving out the values eg vpc_id, such taht the values can be fetched and could be used anywhere outisde for automation

output "vpc_id" {
  value = aws_vpc.this.id
}

the value which we need to show should be added in the value field.

NOTE: if it contains the dynamic value it will show us "to be fetched" ==> after applying terraform plan

But if we are applyin the terraform apply ===> then go for terraform plan then it shows us the output value here in our case the vpc id 

to get the output diresctly we can give it as "terraform output" command
---------------------------

NAT Gateway
After that in VPC also need to connect the NAT GAteway to our vpc as 

Our application will be in the private subnets and if any dependencies are needed then we need it to fetch from the local or outside then 
For that we need to install the nat gateway in the public subnet such taht it will acts as the bridge between the outer worl and our webserver(application) to fetch the dependencies

We need t create the resource for the nat gateway
NOTE:  NAT Gateway is for private subnets but we need to host it on the public subnets ?!
resource "aws_nat_gateway" "example" {
  allocation_id = aws_eip.example.id
  subnet_id     = aws_subnet.example.id

  tags = {
    Name = "gw NAT"
  }

  # To ensure proper ordering, it is recommended to add an explicit dependency
  # on the Internet Gateway for the VPC.
  depends_on = [aws_internet_gateway.example]
}

For NAT Gateway we need to asssign a Public IP i.e. a elastic IP and that is billable it costs our usage per seconds

IN NAT Gateway it also has allocation_id which has the dependency on the following such as elastic_ip i.e. elastic_ip so we need a resource for taht also 
resource "aws_eip" "byoip-ip" {
  domain           = "vpc"
  public_ipv4_pool = "ipv4pool-ec2-012345"
}

and change it to this 

remove the ipv4 field as we are not assigning a static ip there

resource "aws_eip" "this" {
  domain           = "vpc" 
}

---------
 resource "aws_nat_gateway" "this" {
  allocation_id = aws_eip.this.id
  subnet_id     = aws_subnet.example.id # we want it for 2 AZ but it will charge us more so to avoid the incursion we get it as 


  tags = {
    Name = "gw NAT"
  }

  # To ensure proper ordering, it is recommended to add an explicit dependency
  # on the Internet Gateway for the VPC.
  depends_on = [aws_internet_gateway.example]
}

subnet_id     = aws_subnet.example.id  here we will be adding the NAT GAteway in the 1 AZ only and not any other one to reduce costing in AWS

so for getting only one value we will take it as

we want to create the nat gateway in 1 AZ only

In TF there is genearlly the implicat dependency followed in like 

FIrst there will be the vpc creation then only the subnets we can create or viceversa is not possible in our case.


 resource "aws_nat_gateway" "this" {
  allocation_id = aws_eip.this.id
  subnet_id     = aws_subnet.example.id # we want it for 2 AZ but it will charge us more so to avoid the incursion we get it as 


  tags = {
    Name = "gw NAT"
  }

  # To ensure proper ordering, it is recommended to add an explicit dependency
  # on the Internet Gateway for the VPC.
  depends_on = [aws_internet_gateway.example]
}

But sometimes we need to pass some external implications

So in the above code the natgateway should only be created afterthe internet gateway is created and that is carried in the "depends_on" meta_argument for explicait dependency


VERY IMPORTANT NOTE: If wanted to add the Route table assosisation remember to terraform apply first before adding the Route Table assosiation

1st run with natgateway and o/p and then later run it after adding route table assosiation
 

################################## END DAY 13 ################################################

Earlier  as we obsereved we were facing this issue

VERY IMPORTANT NOTE: If wanted to add the Route table assosisation remember to terraform apply first before adding the Route Table assosiation

1st run with natgateway and o/p and then later run it after adding route table assosiation
 

So according to the discussuion we can observe that the we do not need to  comment out and then run the infra without the assosiate route table 

As it was unable to fetch the private and public subnet at the time of execution then what we can do is to use the MAP instead of SET 

Earlier Version: for_each       = toset([for each_subnet in aws_subnet.private_subnet : each_subnet.id])
Chnaged Version: for_each       = { for index, each_subnet in aws_subnet.private_subnet: index => each_subnet.id }



Statefile and best practices

1. State File acts as single source of truth for Terraform configuration
2. Whatever is declared in the Terraform File is known as the Desired State !! 
3. Whatever is created in the cloud infra is called as Current State

So we always define what is desired in the current state in the Terraform File and we get the current state in the cloud infrastructre

Q. How terraform comapres the Desired State with the Curent State ?

With the help of STATE FILE only.

1. State File contains everything the Key value pairs.
2. It contains granular level details about the infrastruccture

terraform state list --> It lists out all the reosurces which has been provisoned by the Terraform

Q. What if we remove any resource from the Terraform state file ?

terraform state rm <resource_name> eg. terraform state rm aws_eip 

again running terraform plan

terraform plan 

Working: It will again try to install the eip and the earlier installer eip will now be destroyed

NOTE: While removing anything the resource already lying eg. The earlier elastic ip it will not be tracked so we need to delete that resource manually or else it will incurr costing

terraform state -help => States the list we can use with the teraform state

terraform state show <resource_name> => gives details about the resource
--------------------

Backend: It defines where terraform store its State Data files

NOTE: Add the backend in the provider.tf 

This lets multiple people access the state data and work together on that ollection of infra resources

Refertence Links : https://developer.hashicorp.com/terraform/language/settings/backends/configuration

We can store in consul ==> it is a hashicorp product and stores in the key value pair

What all can be part of backend ?

1. azurerm
2. consul
3. gcs
4. kubernetes
5. local
6. oss
7. pg
8. s3
9. etcd (kubernetes)


Consider s3 as storage (Simple Storage Service)

It is object storage service 

Object Storage 
Block Storage ==> supported by san devices
File Storage ==> 


Media files could also be directly stored onto the s3 bucket
s3 as artifactory can also be used.

s3 is a global service then it should be having a unique name acros the world

we need to get the region for the s3 bucket ==> Why ? ==> S3 is global but the bucket should be present in certain region

we need to maintain the version for the s3 bucket also for mainitainance and tracking purpose

in s3 key is the filename

  backend "s3" {
    bucket = "terraformstatefiles3backend"
    key = "teraform-state-file-backend-s3"
  }

aks us to add a region we can add directly in the backend the region as a field then

  backend "s3" {
    bucket = "terraformstatefiles3backend"
    key = "teraform-state-file-backend-s3"
    region = "us-east-1"
  }

While createing it on the AWS we should consider

to uncheck the  "Block Public Access settings for this bucket"

Enabling the bucket versioning  

Disable the Bucket Key

Fields in the backend "s3" adn their functions 

1 .bucket =>  gives out the names of the bucket
2. key => It is nothing but the file name and state file will be stored in it. Do not need to create it TF will take care to create it.
          eg key = "terraform-state-file-june-23"
3. region ==> The region where the s3 bucket we want to store eg. us-east-1

Why we need to reinitailise ?

As we are storing the data / tfstate file in the backend i.e storing in the s3.
So the statefile which is avilable in the local right now we want to make it avilable in the s3 as well.
So we need to infrom the plugin "terraform provider" that we have made the change of storing the state file in the s3 bucket provided by user in the configuration

For thuis purpose we need to initialise
terrafrom init ==> used to initialise the creation

still not run as it do not have permissiopn ==> What to do ? It needs to assume the role in the backend in the terraform block

As mentioned in the provider.tf file ==> It has first the terraform block and then the aws provider block where we have assumed the role ==> so we need to explicitly add the role_arn field to the terraform block and it will give it the required permission


terraform init ==> create the state file tfstatefile

Try destroying it ? 

terraform destroy it will destroy the gebnerated tfstate file in the s3 bucket also 

terraform state pull
################################################


#####################################Day 15 ##########################################

As we have commented the bucket part in the provider.tf ==> we need to renitialise it using the terraform init
Again applying terraform plan 

We face an error like 

 Reason: Unsetting the previously set backend "s3"
│ 
│ The "backend" is the interface that Terraform uses to
│ store state,
│ perform operations, etc. If this message is showing up,
│ it means that the
│ Terraform configuration you're using is using a custom
│ configuration for
│ the Terraform backend.

What it means is that there was earlier a resource in the aws s3 with some name and as now we are omitting it so we need to reconfigure 

So for the rconfiguration it can be done with the help of ==> terrafform init -reconfigure  or terraform init -migrate-state

Based on use case: 

terrafform init -reconfigure ==> It will completely reconfigure and locally created state file will be considered.
terraform init -migrate-state  ==>  The state from the state file which is availabel in the s3 will be migrated to the local.

If state file resources are available in the different location them yhe migrate state is preffered.
If we want to create everything from the scratch from the new state store we should then consider the reconfigure


In our case we are  migrating the state 
terraform init -migrate-state

If we try to apply now from the vscode terminal then it will be 

teraform apply --auto-approve

It will run in the external terminal but will throw out the error for our local system in vscode

What error does it pass to the local system is 

Error: Error acquiring the state lock
│ 
│ Error message: resource temporarily unavailable
│ Lock Info:
│   ID:        aea5fe89-e8c0-d0bc-f472-1023f59130d4
│   Path:      terraform.tfstate
│   Operation: OperationTypeApply
│   Who:       aditya@aditya-Inspiron-3576
│   Version:   1.4.6
│   Created:   2023-06-21 10:49:29.294483941 +0000 UTC
│   Info:      
│ 
│ 
│ Terraform acquires a state lock to protect the state
│ from being written
│ by multiple users at the same time. Please resolve the
│ issue above and try
│ again. For most commands, you can disable locking with
│ the "-lock=false"
│ flag, but this is not recommended.

NOTE: In TF the same operrtion should not happen at same time from two diferent locations

NOTE:SO till the time terrafrom apply is running on extrnal terminal; it cannot be runned from another location

NOTE: It is used to prevent duplicacy or changes from two different location at the same time

NOTE: In the local state storage it is by default available.

Q. What if I want to initialise whatever in teh state file in localshould be present in the s3 also>
use terraform init -migrate-state



But when we run tghe terraorm apply -migrate-state on boith the terminal now it is running and do not give the error related to state lock ? WHY ?

==> When we have the state file available locally then terraform takes care of locaking the state so that multiple places we cannot perform the operation related to terraform
But when the backend is stored in the Remote then we need to take care of the locking as well.  


So we need the locking mechanism for the state locking ? Is there some method so that from one palce it could be applied ?

We can use the DynamoDB Table for state locking


In S3 backend ==> Dynamo DB State locking

1. Create the DynamoDB Table ==> It is a NO SQL Database. It not only stores the value in structured and unstructured data can completely managed by AWS
table name as "terraformstatetable"

NOTE: A "Partition Key" also must be added by name "LockID" and type should be a "string"


NOTE: We do not need to take care of the integration of the AWS S3 and DynamoDb integration after providing the partition key as LockID

Add it to the backend block in terraform block in the provider.tf

dynamodb_table = "terraformstatetable" ==> dynamodb_table ="name of the file"


  backend "s3" {
    bucket   = "terraformstatefiles3backend"
    key      = "terraform-state-file-june-23"
    region   = "us-east-1"
    role_arn = "arn:aws:iam::972348856143:role/stsassumerole"
    dynamodb_table = "terraformstatetable"
  }

Now try to run ==> But first we need to initialise the terraform file with terraform init -reconfigure

try running it on both the browsers

It naow enables the state lcoaking for us 
-------
Now we are going to move towards the autoscaling group ==> we need to create the insatcnes for webserver and the application server and alos need to create the RDS

WE can create using the EC2 instance and another one using the Auto scaling group
-----
Before than we have "locals"

we can define our local variable in the "locals block"

locals vs variables ??

Local acts as input variable only and based on variables we can create a condition
We can define local variable in the locals block

- In variables we can have the value of particular datatype; but I want some customization it should also have some extra charecters in the end.

eg we want to add the vpc_name with some other info then we need to make use of the locals block.

eg creating locals.tf in which it contains 
locals {
  vpc_name_local = "${var.vpc_name}-lwplabs"
}

In the variable we cannot do such things and so we need to make use of locals we can only make use of default value it will only be taken if values not provided by user.

Scenario: If for 2 AZ we want some condition and for 4 AZ we want another condition then ?

It can be done with the help of this solution 
locals {
  vpc_name_local = "${var.vpc_name}-lwplabs"
  az_required = var.env == "prod" ?  slice(data.aws_availability_zones.this.names, 0, 2) :  slice(data.aws_availability_zones.this.names, 0, 4)
}


Now we want to reflect these changes on our vpc so we need to make changes on network.tf

resource "aws_vpc" "this" {
  cidr_block           = var.cidr_for_vpc
  instance_tenancy     = var.tenancy
  enable_dns_hostnames = var.dns_hostnames_enabled
  enable_dns_support   = var.dns_support_enabled
  tags = {
    Name = var.vpc_name
  }
}

In tags we need to make a change here like 

  tags = {
    Name = local.vpc_name_local
  }

  Remember while retriving the data from local we use local but not locals.

----------------------------------
We will createthe EC2 insatcneresource and another one with the auto scaling

create webserver.tf  => search for aws instance resource terraform,

resource "aws_network_interface" "foo" {
  subnet_id   = "${aws_subnet.my_subnet.id}"
  private_ips = ["172.16.10.100"]

  tags = {
    Name = "primary_network_interface"
  }
}

create the ec2 insatnce using aws console

webserver =>  amazon linux => create a key-pair(ultimate-cicd used here)

  ami   = "ami-022e1a32d3f742bd8"
  instance_type = "t2.micro"
  key_name= "ultimate-cicd-pipeline"

we we are doing is we are having a variable with name web_server

in variable.tf ==> 

variable "web_server_name" {
  type = string
  description ="name of the ec2 instance in which webserver is present"
}

we should take the web server name from the user so add it into tfvars.tf file

web_server_name = "terraform-webserver"

adding it by local we need custom name to it

locals {
  web_server = "${var.web_server_name}-terraform"
} 


To make use of the locals in the webserver.tf file for the web server created by aws instance

tags {
  Name = "lcoal.web_server"
}

In this we also need to provide the subnet id and we do not want to create anything in public but the private so we can make use of subnet_id of private

subnet_id = aws_subnet.private_subnet

But it would not accept it as it would require only string for this so we get

to make use of for loop

subnet_id = [for each_subnet in aws_subnet.private_subnet : each_subnet.id]

But we need to create it only on 1 part so we can get to pick using the "element" function

subnet_id = element([for each_subnet in aws_subnet.private_subnet : each_subnet.id],1)

but we have allowed all the teraffic in our case in our ingress(Inbound Rules) but in real case sceanrio there is not such a case so we need to create a security group also


search aws security group resource terraform

resource "aws_security_group" "allow_tls" {
  name        = "allow_tls"
  description = "Allow TLS inbound traffic"
  vpc_id      = aws_vpc.main.id

  ingress {
    description = "TLS from VPC"
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.main.cidr_block]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_tls"
  }
}

in webserver.tf

We want to mke it open fo the webserver so we need to add the port 80(apache/httpd) in ou ingress Rules

ingress {
    description = "TLS from VPC"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.main.cidr_block]
  }

  We have not creaated the load balancer yet so we should open the cidr block for this only

ingress {
    description = "TLS from VPC"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.this.cidr_block]

  }

Also we need to add nother inbound rrule for SSH purpose so add port 22 for that so we need nother ingress block

ingress {
    description = "TLS from VPC"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.this.cidr_block]
    
  }

  Gebnarally the traffic to the webserver should be connected via the Load balancer and with that we need to connect it to the webserver 

  But can we directly conect to our webserver ?

  No we cannot connect directly to our webserver as its in Private Subnet and we need the help of bastion server for that

  outer people wanting to access our webserver can only access via

  aws_nat_gateway --> loadbalancer ==> bastion host ==> webserver in the private subnets

  IMPOTANT NOTE:: Also the NAT Gateway will block all the reqest coming from outside if there is absence of bastion host as no one can directly aaccess the webserver in the private subnet.

  All the dependencies of the web server will be installed by that.

  Also make a note that from connecting via the Bastion host we need to have
  
  > We need to have the pem key file of the web server instance as it will be required and also

  > To connect it via ssh and pass its Pivate IP address (NOT PUBLIC)

  
egress represnt all the outbound rules and it should be for all the traffic.


  vpc_security_group_ids = [aws_security_group.this].idss

  as we are creating the instances in a VPC we are makiing use of this 

  if not ec2 clasic and the vpc default we can make the use of "security_groups"

  

################################### END DAY 15 ################################################
