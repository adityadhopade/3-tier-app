WE can save the credentials wioth the help of the 

1. By putting out credentials directly in TF (Not recommneded)
2. By adding the profile in the aws credentials (used widely)
3. Adding details via Env variables (Mostly used in Pipelines Framework)

    export AWS_ACCESS_KEY_ID = "your key"
    export AWS_SECRET_ACCESS_KEY = "your secret access key"
    Run terraform plan .. It must work

4. By installing TF on instances we can assign a role to the ec2 instance so that we can use it more securely and then access directly (Widely used; Best practice; more secured)

############    Day 6  USing STS  ################### 

STS [Secure Token Service] in AWS ; It is used to generate a token for the short time. For Min time we can set ourselves; Max for 12 hours.
It gives access key; secret key and Session Token generation is provided with the expiration date


Commands followed during the creation of Secure Token Service execution is as follows
 
1. create a user "stsuser" with no permissions --> no policy attached --> add it to the aws credentials file with the same profile name 	 
  aws configure --profile stsuser
  cat ~/.aws/credentials 
  aws s3 ls (Fail)
  
2. Create a role --> For aws account --> Admin Access(Highest Privileage) --> name as "stsassumerole"
  aws s3 ls (Fail)

3. Need to integrate the created User with the role

a. On user End --> Permission --> Need to add a inline policy --> Using template for using the IAM ROle in the AWS CLI --> Change the arn with the arn of the role we need to assume. --> name it as "stsassumerolepolicy"

b. On the Role End --> It needs to trust the user so refer to trust relationships --> Exactly the same as in the https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-role.html

aws s3 ls (Fails) --> Its still assuming that we are executing it with the current user but we need to implement it via the role(that we recently created) --> Follow the command  

a. aws sts assume-role --role-arn arn:aws:iam::972348856143:role/stsassumerole --role-session-name accesss3
  
b. aws sts get-caller-identity --> It shows out the current user in my case gives terraform user
  
To configure which profile to be used 
  
Just edit out the file using -->code ~/.aws/config
Add a profile with the fields role_arn (arn of the role), source_profile(the credential for which we want to assume the role)
  
[profile sts]
role_arn = <arn of the role>
source_profile = <credential for which we want to assume role = stsuser in my case>  
  
For source profile credentails we can verify it via the --> cat ~/.aws/credentials
  
  
then if everything is configured

aws s3 ls --profile sts --> Lists out the s3 bucket lists

This should work as it was working fine for some time now

Try creating any new user--> add credentials in the aws credentails folder --> Do not give any permission to the user then ty accessing suing the command 

aws s3 ls --profile new_user (It will not work)

#########################  END Day 6 ####################################### 

########################## Day 7 Creating VPC via Terraform [7 June 2023] ###############################################

For multiple account the role is configured and centralized account where user is configured then how to configure it suing the Terraform

just add the assume_role block in the provider.tf file

assume_role {
  role_arn = "your_arn"
  session_name = "your seesion name" [It is similiar to as that of the [aws sts --role-arn <arn of the role to be assumed> --role-session-name <your session name>] Also remember the session maxes out at 12 hrs so need to create a new token after that]
}

How to actually create a VPC using the TF ?

1.In VPC there are lots of fields known as Arguments which are needed to fill to host VPC {Some mandatory or Optional}

2. In tf planning if we do not want hassle to cross check then give terraform plan --auto-approve

3. For generating VPC we need the bare minimum as [cidr_block]  OR [ipv4_ipam_pool_id] (Which inturn requires the ipv4_netmask_length to calulcate the cidr range)

While creating a resource block 

resource "aws_vpc" "main" {}

aws_vpc -- represents what type of resource is expected by the provider to create.
main -- represents the name of that resource but standards should be followed like giving it names like this, main, my_vpc, main_vpc

To create vpc we need to provide the cidr ranges which can be easily get by --> search  private ip ranges and based on the requirement add a class range

to save a plan in teraform just add the --> terraform plan -out sample_name.out (File will be generated with such name)

to execute that just use --> terraform apply "sample_name.out" (It would create the vpc resource in aws console we caqn verify)

########################### End Day 7 [7 June 2023] #########################################

########################### Day 8 ##################################################
######################################### END Day 8 ########################################

###########################  Day 9 Creation of Aws 3-tier architecture by using for each, Count in terraform ,  some basic git commands cm [8 June 2023] #########################################

#1st Iteration

# resource "aws_subnet" "this" {
#   # for_each   = toset(["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", " 192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"])
#   for_each   = { "1": "192.168.0.0/27", "2":"192.168.0.32/27", "3": "192.168.0.64/27", "4": "192.168.0.96/27", "5": "192.168.0.128/27", "6": "192.168.0.160/27"}
#   vpc_id     = aws_vpc.this.id
#   cidr_block = each.value

#   tags = {
#     Name = "Main_${each.key}"
#   }
# }


# resource "aws_subnet" "this" {
#   # count  = var.no_of_subnets
#   count = length(var.cidr_subnet)

#   #length is used to find the length of list
#   vpc_id = aws_vpc.this.id
#   #element helps us to traverse through the list one element at a time
#   # In terraform a list can have same type of dataype; as in python
#   cidr_block = element(var.cidr_subnet, count.index)
#   tags = {
#     Name = "subnet-${count.index}"
#   }
# }

variable "no_of_subnets" {
  type        = number
  description = "Number of subnets to be created"
  default     = 6 #If variable is not passed then waht value needed to be considered
}

variable "cidr_subnet" {
  type        = list(string)
  description = "List of CIDR range in Subnets"
  default     = ["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", "192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"]
}

#terraform console used to checkout the functions functioanlity of TF in the console itself.

# Now we need to create public and private subnets as per our discussions

# resource "aws_subnet" "this" {
#   # for_each   = toset(["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", " 192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"])
#   for_each   = { "public_subnet_1" : "192.168.0.0/27", "public_subnet_2" : "192.168.0.32/27", "private_subnet_1" : "192.168.0.64/27", "private_subnet_2" : "192.168.0.96/27", "private_subnet_3" : "192.168.0.128/27", "6" : "192.168.0.160/27" }
#   vpc_id     = aws_vpc.this.id
#   cidr_block = each.value
#   availability_zone = 

#   tags = {
#     Name = "Main_${each.key}"
#   }
# }

#for_each is widely usedas compared to the count

#But we need to consider the point of different AVALABILTY ZONES
# As per our requiremnts we need to have the 2 Avalibilty zones

#Search for it in the arguments refrences

#pass avalibility zones but it takse string as input and we need to provide the list here

#we need to pass 1 public and 2 private in each AZ i.e. for 6 subnets we will consider here 2 AZ i.e. us-east-1a, us-east-1b

# 1 way is by creating 1 subnet for AZ 1 and another subnet for AZ2

resource "aws_subnet" "subnet_az1" {
  # for_each   = toset(["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", " 192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"])
  for_each          = { "public_subnet_1_az1" : "192.168.0.0/27", "private_subnet_1_az1" : "192.168.0.32/27", "private_subnet_2_az1" : "192.168.0.64/27" }
  vpc_id            = aws_vpc.this.id
  cidr_block        = each.value
  availability_zone = "us-east-1a"

  tags = {
    Name = "Subnet_${each.key}"
  }
}

resource "aws_subnet" "subnet_az2" {
  # for_each   = toset(["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", " 192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"])
  for_each          = { "public_subnet_1_az2" : "192.168.0.96/27", "private_subnet_1_az2" : "192.168.0.128/27", "priavte_subnet2_az2" : "192.168.0.160/27" }
  vpc_id            = aws_vpc.this.id
  cidr_block        = each.value
  availability_zone = "us-east-1b"

  tags = {
    Name = "Subnet_${each.key}"
  }
}

#But the above process goes through the lot of code repetation


########################### End Day 9 [8 June 2023] #########################################

########################## Day 10 [] #######################################################

Disusions about the JIRA board

To create a branch using the story provided we should use the following like [orgname-jira.story.no-branchname] this is the standard followed throughout

create variable.tf
variable "cidr_for_vpc" {
    description = "the cidr range for VPC"
    type = string
}

#If we want to input the value from the user then we do not need to add the "default"


enable_dns_support - (Optional) A boolean flag to enable/disable DNS support in the VPC. Defaults true.

enable_dns_hostnames - (Optional) A boolean flag to enable/disable DNS hostnames in the VPC. Defaults false.

What if we need ourr resources into multiple avalilale zones then we need to add a Data Source to feed that how many number of avalibilty zones are there by creating data.tf file

Data source is special kind of block like resource block in TF which actually retrieve the information as per the defination

eg how many AZ available in north virginia

i.e. basically it doesnt create any resource but it actually helps us out in the info about anything available in resources like aws, azure, gcp



eg in this vpc we nned to create eks cluster but its in another foleder with the help of datasource we can get the id of the vpc and pass it to the eks cluster

add data.tf file and search for the datasource in terrafrom get this template from the below
From here we will get the final 

data "aws_availability_zones" "this" {
  all_availability_zones = true
  filter {
    name   = "opt-in-status"
    values = ["opt-in-not-required"]
  }
}

Creating 2 blocks for the private subnet and the public subnet

remove the existing code

resource "aws_subnet" "subnet_az1" {
  # for_each   = toset(["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27", " 192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"])
  for_each          = { "public_subnet_1_az1" : "192.168.0.0/27", "private_subnet_1_az1" : "192.168.0.32/27", "private_subnet_2_az1" : "192.168.0.64/27" }
  vpc_id            = aws_vpc.this.id
  cidr_block        = each.value
  availability_zone = "us-east-1a"

  tags = {
    Name = "Subnet_${each.key}"
  }
}

resource "aws_subnet" "subnet_az2" {
  for_each          = { "public_subnet_1_az2" : "192.168.0.96/27", "private_subnet_1_az2" : "192.168.0.128/27", "priavte_subnet2_az2" : "192.168.0.160/27" }
  vpc_id            = aws_vpc.this.id
  cidr_block        = each.value
  availability_zone = "us-east-1b"

  tags = {
    Name = "Subnet_${each.key}"
  }
}

Suppose we need to create the private subnets fopr all tha availibility zones then add

for_each = { for index, az_name in data.aws_availability_zones.this.names : index => az_name }

for the for_each we are passing out the map datatype as it supports the map and set datatype
index - gives out the index of the subnets and 
az_name -- will give us out the avalibility zones 

data.aws_availability_zones.this -- It will let us know all the availibility zones in the particular region region
eg for us-east gives out 6 AZ
for mumbai fives out 3 AZ

now to get the xact name if the az we could use the attribute reference like "name" 
 data.aws_availability_zones.this.name

Now by this it will only fetch if for the single subnet but we want it for multiple subnets 

so here we requie for loops and In Terraform only for loop is supported not the while loop

for_each = { for index, az_name in data.aws_availability_zones.this.names : index => az_name } // it will give us ourt the o/p inform of map datatype

For map datatype it is a set value os requires in form key : value pairs

so we should add for the for loop as index => az_name

in 1st iteration it will act like for az_name in [us-east1a, us-east-1b...]
and it will iterate throughout the list which we get from the data source

index represents the index of the subnet 
az_name represents the names of the availability_zone


What if we do not want all the avalibilty zones but only some of them like us-east-1a, us-east-1b

Then here wecan follow a modular approach like using the "locals"

using locals we can asign then names to an expression so that we could use names multiple times within a module instead of repeating the expression

NOTE:  Local values are created by a locals block (plural), but you reference them as attributes on an object named local (singular). Make sure to leave off the "s" when referencing a local value!



locals {
  service_name = "forum"
  owner        = "Community Team"
}

and to retrieve vvalues form it 

resource "aws_instance" "example" {
  # ...

  tags = local.common_tags
}

we can use it in our case as 

data "aws_availability_zones" "this" {
}

locals {
  sorted_availability_zones   = sort(data.aws_availability_zones.all.names)
  selected_availability_zones = toset([
    local.sorted_availability_zones[0],
    local.sorted_availability_zones[1],
  ])
}

resource "aws_subnet" "public_subnet" {
  for_each = local.selected_availability_zones

  vpc_id = aws_vpc.this.id
  availability_zone = each.value
}

resource "aws_subnet" "private" {
  for_each = local.selected_availability_zones

  vpc_id = aws_vpc.this.id
  availability_zone = each.value
}

But this would onlyu work if we want the consecutive AZ and from the start


2nd Approach could be as follows like 

data "aws_region" "this" {}

locals {
 az1 = "${data.aws_region.this.name}a"
 az2 = "${data.aws_region.this.name}b"
}


The cidrsubnet(prefix, newbits, netnum) Syntax

prefix: it gives the cidr_range of vpc
newbits: how many subnets we need to create let us say 6 i.e. it can be satisfied with the help of 2^3 = 8 so 3 will act as the newbits
netnum(Important): 

eg 10.0.0.0/24 for creating 6 subnets --> 2^3 ie 3 will be new bits
3 bits we will be giving for subnet masking
10.0.0.0/24 in binary representation

8bits + 8bits + 8bits + 8bits = 32 bits 
24 bits already assigned to network
8 bits remains for the host -- out of which 3 we are adding to the host (As a part of adding the subnet)

Only 5 bits are available to the host now

So now 3 bits can be raging from 000 to 111
start from rightmost to the left
2^0 * 1 = 1 (maximum value it can have at that unit position)
2^1 * 1 = 2 (maximum value it can have at that tens position)
2^2 * 1 = 4 (maximum value it can have at that hundredth position)

Adding it up brings out 1 + 2 + 4 = 7(Max value it can obtain) 

7 acts as netnum here


git status --short // gives status befor staging

git log --oneline // gets logs in single line
########################## End Day 10 [] #######################################################

########################## Day 11 - cidrsubnet function in TF #################################

cidrsubnet(prefix, newbits, netnum)

calculating the cidr subnet values
prefix is nothing but the vpc_cidr range so we can obtain it like var.cidr_for_vpc (From the variables.tf file) OR we could also use the aws_vpc.this.cidr_block (valid way)

now we need to form a logic to get the newbits(how many subnets we need to create let us say 6 i.e. it can be satisfied with the help of 2^3 = 8 ;So 3 will act as the newbits)
logic : we can do that using length function of TF --> length(data.aws_availability_zones.this.names)

For the newbits => If subnets 3 ; It is satisfied by 2^2 = 4 ; 2 will be the newbits
                   If subnets 6 ; It will be satified by 2^3= 8 ; 3 will be the newbits


length(data.aws_availability_zones.this.names) > 3 ? 4 : 2 (USing ternary operator)


eg if cidr= 10.0.0.0/24
for 3 subnets => 2^2 = 4 staisfies; 2 newbits

cidr will now be 10.0.0.0/26

TASK: But we will not know the how many avaklibility zones would be there; but we need each AZ should contain a private subnet

NOTE: AZ will not be greater than 8 for now(Max to max in aws till now)

if suppose there we have 6 AZ then in that case; as per our condition we require each AZ should contain each private subnet so we reuire 6 subnets

length(data.aws_availability_zones.this.names) gives us 6

Example: for 6 subnets how much the new bits are required ? Calculate

6 => 2^ 3 = 8 ; 3 new bits will satisy our needs

Earlier: length(data.aws_availability_zones.this.names) > 3 ? 4(Indicates true part) : 2 (Indicates false part)
Changed: length(data.aws_availability_zones.this.names) > 4 ? 3(Required newbits) : 2
         
Decoding Logic   In our case for 6 subnets =>  6 > 4  its true so it will trigger true part i.e. 3
As per our ternary condition we need to make some changes here like
NOTE: It would work for the requirement form 5/6/7/8 subnets in total


FOr netnum calculation

eg 10.0.0.0/24 for creating 6 subnets --> 2^3 ie 3 will be new bits
3 bits we will be giving for subnet masking
10.0.0.0/24 in binary representation

8bits + 8bits + 8bits + 8bits = 32 bits 
24 bits already assigned to network
8 bits remains for the host -- out of which 3 we are adding to the host (As a part of adding the subnet)

Only 5 bits are available to the host now

So now 3 bits can be raging from 000 to 111
start from rightmost to the left
2^0 * 1 = 1 (maximum value it can have at that unit position)
2^1 * 1 = 2 (maximum value it can have at that tens position)
2^2 * 1 = 4 (maximum value it can have at that hundredth position)

Adding it up brings out 1 + 2 + 4 = 7(Max value it can obtain) 

7 acts as netnum here [Ranges from 0 to 7]

so net num should move form 0 to 7 

We cannot us for loop as its already in use ; no foreach loop as its already in use;

But we can see as we arre calculating the map in the for_each
for_each   = { for index, az_name in data.aws_availability_zones.this.names : index => az_name }

as the for_each loop only supports map and set and here it is providing us with the map ; so we could make use of the key value pair in it like {"1": "us-east-1a", "2": "us-east-1b"...}
as we just wanted the number so we can just loop in the by using "each.key"

Similarly to find the avalibility zones we have can fetch it as ==> each.value

-----------
Scenario: So now in the case of the public subnet we need to change the IP Adresses as it would result into the Ip Address Conflict (as private and the public subnet should not contain the same IP)

here we are creating 6 subnets in total for public and private subnets

for 6 subnets new bits will be 3 (Calculated above)

Now our biggest challenge will be the asigning diffrnt ranges for public and private Subnets

length(data.aws_availability_zones.this.names) > 4 ? 3(Required newbits) : 2
Change1 : length(data.aws_availability_zones.this.names) > 3 ? 3(Required newbits) : 2 (It will fail when the subnets are less than 3 here as it would require 2 as newbits)
Change2 : length(data.aws_availability_zones.this.names) > 3 ? 4(Required newbits) : 3 ()
Change3: Now we need to find the netnum such taht the Ips do not coincide with each other

         length(data.aws_availability_zones.this.names) > 3 ? 4 : 3, each.key+length(data.aws_availability_zones.this.names)    

         Let us consider for the
         Public ==> 1-3 subnets should be there
         Private ==> 4-6 subnets should be there so by adding this length(data.aws_availability_zones.this.names) > 3 ? 4 : 3, each.key+length(data.aws_availability_zones.this.names) it would work as charm    

Or Change 4: simply we could just add it into the index of the private subnets so it will just iterate it from that number of index

        Initially index=0 , length(data.aws_availability_zones.this.names) > 3 ? 4 : 3, each.key+length(data.aws_availability_zones.this.names) let be 3
        index+(data.aws_availability_zones.this.names) > 3 ? 4 : 3, each.key+length(data.aws_availability_zones.this.names) = 0+3 =3

        next iteration ==>  1 + 3 = 4
        next iteration ==>  2 + 3 = 5
        next iteration ==>  3 + 3 = 6
        

########################## END Day 11 ########################################################

############################## Day 12 Add dynamic variables and Route Tables########################################################

Scenario: In our earlier case we have built 6 oublic and 6 private subnets but we want to limit it for each AZ 1 public and 1 private subnets

For this we can use function slice

slice -- Syntax: slice(list, start index, end index)

NOTE: It would only work on the list dataype and end index will not be considered in the list


adding the slice in the "private_subnet" for loop here we will get the follwing { for index, az_name in slice(data.aws_availability_zones.this.names, 0, 2) : index => az_name }
so by the steps above it will create 2 Avalibilty zones as per our condition we require same number of subnets as avalibilty zones so we get 2 subnets in the private
same piece of chaneges for the "Private subnet" also ==> 2 public subnet here also created (By above logic)



For 4 subnets(2 public and private) the new newbits will be as follows

2^2 = 4 satisfies ==> 2 will be the netnum then

so we need to change the condition of our ternary operator as follows

Earlier: length(data.aws_availability_zones.this.names) > 3 ? 4 : 3
Changed: length(data.aws_availability_zones.this.names) > 3 ? 4 : 3
          4 < 3 ? 2 : 3
so now it will traverse from the 0th index avalilbilty zone eg us-east-1a ; 1st index: will be us-east-1b; it will be there only  
Also the index+(data.aws_availability_zones.this.names) gives out some problem so we can move that length(data.aws_availability_zones.this.names) to each.key(As seen on day 11)

-------------------------
To passs the variables in here we can do it by many ways 

way1 : is by passing variables

terraform apply --var "cidr_for_vpc=10.0.0.0/24" --var "vpc_name=vpc_3tier"

terraform apply --auto-approve -var="cidr_for_vpc=10.0.0.0/24"

way2 : It is by adding the file like [network.tfvars] and add the variable here

cidr_for_vpc= "10.0.0.0/24"
vpc_name = "terraform_vpc"
NOte: we can also define(override) the existing ones which ahve also defined in their default fields
eg tenancy = dedicated

While destroying even if it takes the var argumnets it is of no use here in our case

terraform plan --auto-approve  -var-file = "network.tfvars"

if do not want to write the dediacated file name in -var-file then just change the name of the file to the following format

network.auto.tfvars ==> try to run using terraform apply then it would automatically consider your file and the variables in it will get passed

way3 : By adding the file terraform.tfvars (nod need to ad auto here as it has higher preceddence)

It acts a the default file for the terraform conisdering all thetfvars file.

During destroying aslo it doesnt take any input after apssing it the terraform.tfvars file

way4 : By adding the Environment variables for the variables

export TF_VAR_cidr_for_vpc = "10.0.0.0/24"
export TF_VAR_cidr_for_vpc="10.0.0.0/24"

NOTE: But once the session is over (TErminal shuts dowm) the ENV_VARIABLES get destroyed


Precedence order (last has the highest priority)

Environment variables
The terraform.tfvars file, if present.
The terraform.tfvars.json file, if present.
Any *.auto.tfvars or *.auto.tfvars.json files, processed in lexical order of their filenames.
Any -var and -var-file options on the command line, in the order they are provided. (This includes variables set by a Terraform Cloud workspace.)



Resource for Route Tables

We need to add 1 default route table. If we aare making change in the route table then all the previous routes will be scraped and only new ones would get considered

// priavte Route table 
resource "aws_default_route_table" "this" {
  default_route_table_id = aws_vpc.this.default_route_table_id

  # WE do not need the route for the local by default it will be added

  tags = {
    Name = "private_routetable_${var.vpc_name}"
  }
}

// Public Route Table we need to provide the internet gateway such taht the access should be allowed by everyone

Also for the public route table we need to add the internet_gateway as the ressource and should also check for the required dependencies of the internet gateway which is (the vpc_id in our case)

It should have target as internet gateway(that we need to create using the resources) and destination as 0.0.0.0/0 (Open for all i.e. it specifies all networks)


What is the 0.0.0.0/0 ?
The default route in Internet Protocol Version 4 (IPv4) is designated as the zero-address 0.0.0.0/0 in CIDR notation, often called the quad-zero route. 
The subnet mask is given as /0, which effectively specifies all networks, and is the shortest match possible.


resource "aws_route_table" "this" {
  vpc_id = aws_vpc.this.id

  # WE do not need the route for the local by default it will be added

  route {
    cidr_block = "0.0.0.0/0"
    # in public rt we require the internet gateway also
    gateway_id = aws_internet_gateway.this.id
  }

  tags = {
    Name = "public_routetable_${var.vpc_name}"
  }
}

#We need to add it and what arguments are required in it with the attribute refrences

resource "aws_internet_gateway" "this" {
  vpc_id = aws_vpc.this.id

  tags = {
    Name = "igw-${var.vpc_name}"
  }
}

making those changes we will get the error below
│ Error: creating Route in Route Table (rtb-062e33bad961ed4df) with destination (10.0.0.0/24): InvalidParameterValue: The destination CIDR block 10.0.0.0/24 is equal to or more specific than one of this VPC's CIDR blocks. This route can target only an interface or an instance.


Why is it facing error?

As we are providing the internet gateway we need to make it availble to access the internet i.e. it should be allowed for everyone


so in the public subnet change the field cidr_block we need to change the following as 0.0.0.0/0
now it should work as defined and with no error

 git log --oneline // gives the log of github
############################## END DAY 12 #####################################################

############################### DAY 13 #######################################################

WE have created the public and private route table ablove but we have not written the "subnet assosiated" with it; we need to add them 

When routes are created then we need to asosisate the subnets

gebnrally "this" is used when we have some unique resource eg only one vpc (so its better to understand)

If we are not multiple resourcesd then the resource name should be added as standard

search for aws_route_table ==>  we can get the folowing as 

resource "aws_route_table_association" "a" {
  subnet_id      = aws_subnet.foo.id
  route_table_id = aws_route_table.bar.id
}

Add for both the public and the private subnet
#route table assosiation for private  subnets
resource "aws_route_table_association" "private_subnet_assosiation" {
  subnet_id      = aws_subnet.private_subnet.id
  route_table_id = aws_default_route_table.this.id
}

#route table assosiation for public subnet
resource "aws_route_table_association" "public_subnet_assosiation" {
  subnet_id      = aws_subnet.public_subnet.id
  route_table_id = aws_route_table.this.id  # also check if we could add the id or not simply by searching it
}


It will throw out the error ? 

 71:   subnet_id      = aws_subnet.private_subnet.id
│ 
│ Because aws_subnet.private_subnet has "for_each" set, its
│ attributes must be accessed on specific instances.
│ 
│ For example, to correlate with indices of a referring
│ resource, use:
│     aws_subnet.private_subnet[each.key]
╵

How to resolve it then ?

As subnet_id      = aws_subnet.public_subnet.id it is a for_each and we cannot simply calculate it by this

As we do not have single id we have multiple public and private subnet so we need to make changes such taht we can make a refrence to all of the ids ?

We need t o use the "splat expressions" 

It is represented byu the help of the [*] ==> It can be directly used if instead of "for_each" it has "count"

In case of count it will iterate through all the ids like

resource "aws_route_table_association" "private_subnet_assosiation" {
  subnet_id      = aws_subnet.private_subnet[*].id
  route_table_id = aws_default_route_table.this.id
}

#route table assosiation for public subnet
resource "aws_route_table_association" "public_subnet_assosiation" {
  subnet_id      = aws_subnet.public_subnet[*].id
  route_table_id = aws_route_table.this.id  # also check if we could add the id or not simply by searching it
}

BUT as we have the for_each here so we can use the syntax below i.e.

[for o in var.list : o.interfaces[0].name]

resource "aws_route_table_association" "private_subnet_assosiation" {
  subnet_id      = [for each_subnet in aws_subnet.private_subnet: each_subnet.id]
  route_table_id = aws_default_route_table.this.id
}

#route table assosiation for public subnet
resource "aws_route_table_association" "public_subnet_assosiation" {
  subnet_id      = [for each_subnet in aws_subnet.public_subnet: each_subnet.id]
  route_table_id = aws_route_table.this.id
}


But still its not working as we are reciveing it in for expects a string

we can change the logic in it and alter it by adding a for_each loop

for_each = [for each_subnet in aws_subnet.private_subnet: each_subnet.id]

but for_each accepts either map or set
we convert it to the set using "toset"

for_each = toset[for each_subnet in aws_subnet.private_subnet: each_subnet.id]

 subnet_id      = each.key or each.value (Both are valid in the case of the set)


 try to terraform plan and terraform apply --auto-approve

 Check in the route table ==> subnet associations 

 For public route table we ae getting the public subnets attached to it
 For private route table we are getting the private subnet attached to it repectively

----------------
for now lets moveto output.tf file

The output.tf file helps in gving out the values eg vpc_id, such taht the values can be fetched and could be used anywhere outisde for automation

output "vpc_id" {
  value = aws_vpc.this.id
}

the value which we need to show should be added in the value field.

NOTE: if it contains the dynamic value it will show us "to be fetched" ==> after applying terraform plan

But if we are applyin the terraform apply ===> then go for terraform plan then it shows us the output value here in our case the vpc id 

to get the output diresctly we can give it as "terraform output" command
---------------------------

NAT Gateway
After that in VPC also need to connect the NAT GAteway to our vpc as 

Our application will be in the private subnets and if any dependencies are needed then we need it to fetch from the local or outside then 
For that we need to install the nat gateway in the public subnet such taht it will acts as the bridge between the outer worl and our webserver(application) to fetch the dependencies

We need t create the resource for the nat gateway
NOTE:  NAT Gateway is for private subnets but we need to host it on the public subnets ?!
resource "aws_nat_gateway" "example" {
  allocation_id = aws_eip.example.id
  subnet_id     = aws_subnet.example.id

  tags = {
    Name = "gw NAT"
  }

  # To ensure proper ordering, it is recommended to add an explicit dependency
  # on the Internet Gateway for the VPC.
  depends_on = [aws_internet_gateway.example]
}

For NAT Gateway we need to asssign a Public IP i.e. a elastic IP and that is billable it costs our usage per seconds

IN NAT Gateway it also has allocation_id which has the dependency on the following such as elastic_ip i.e. elastic_ip so we need a resource for taht also 
resource "aws_eip" "byoip-ip" {
  domain           = "vpc"
  public_ipv4_pool = "ipv4pool-ec2-012345"
}

and change it to this 

remove the ipv4 field as we are not assigning a static ip there

resource "aws_eip" "this" {
  domain           = "vpc" 
}

---------
 resource "aws_nat_gateway" "this" {
  allocation_id = aws_eip.this.id
  subnet_id     = aws_subnet.example.id # we want it for 2 AZ but it will charge us more so to avoid the incursion we get it as 


  tags = {
    Name = "gw NAT"
  }

  # To ensure proper ordering, it is recommended to add an explicit dependency
  # on the Internet Gateway for the VPC.
  depends_on = [aws_internet_gateway.example]
}

subnet_id     = aws_subnet.example.id  here we will be adding the NAT GAteway in the 1 AZ only and not any other one to reduce costing in AWS

so for getting only one value we will take it as

we want to create the nat gateway in 1 AZ only

In TF there is genearlly the implicat dependency followed in like 

FIrst there will be the vpc creation then only the subnets we can create or viceversa is not possible in our case.


 resource "aws_nat_gateway" "this" {
  allocation_id = aws_eip.this.id
  subnet_id     = aws_subnet.example.id # we want it for 2 AZ but it will charge us more so to avoid the incursion we get it as 


  tags = {
    Name = "gw NAT"
  }

  # To ensure proper ordering, it is recommended to add an explicit dependency
  # on the Internet Gateway for the VPC.
  depends_on = [aws_internet_gateway.example]
}

But sometimes we need to pass some external implications

So in the above code the natgateway should only be created afterthe internet gateway is created and that is carried in the "depends_on" meta_argument for explicait dependency


VERY IMPORTANT NOTE: If wanted to add the Route table assosisation remember to terraform apply first before adding the Route Table assosiation

1st run with natgateway and o/p and then later run it after adding route table assosiation
 

################################## END DAY 13 ################################################

Earlier  as we obsereved we were facing this issue

VERY IMPORTANT NOTE: If wanted to add the Route table assosisation remember to terraform apply first before adding the Route Table assosiation

1st run with natgateway and o/p and then later run it after adding route table assosiation
 

So according to the discussuion we can observe that the we do not need to  comment out and then run the infra without the assosiate route table 

As it was unable to fetch the private and public subnet at the time of execution then what we can do is to use the MAP instead of SET 

Earlier Version: for_each       = toset([for each_subnet in aws_subnet.private_subnet : each_subnet.id])
Chnaged Version: for_each       = { for index, each_subnet in aws_subnet.private_subnet: index => each_subnet.id }



Statefile and best practices

1. State File acts as single source of truth for Terraform configuration
2. Whatever is declared in the Terraform File is known as the Desired State !! 
3. Whatever is created in the cloud infra is called as Current State

So we always define what is desired in the current state in the Terraform File and we get the current state in the cloud infrastructre

Q. How terraform comapres the Desired State with the Curent State ?

With the help of STATE FILE only.

1. State File contains everything the Key value pairs.
2. It contains granular level details about the infrastruccture

terraform state list --> It lists out all the reosurces which has been provisoned by the Terraform

Q. What if we remove any resource from the Terraform state file ?

terraform state rm <resource_name> eg. terraform state rm aws_eip 

again running terraform plan

terraform plan 

Working: It will again try to install the eip and the earlier installer eip will now be destroyed

NOTE: While removing anything the resource already lying eg. The earlier elastic ip it will not be tracked so we need to delete that resource manually or else it will incurr costing

terraform state -help => States the list we can use with the teraform state

terraform state show <resource_name> => gives details about the resource
--------------------

Backend: It defines where terraform store its State Data files

NOTE: Add the backend in the provider.tf 

This lets multiple people access the state data and work together on that ollection of infra resources

Refertence Links : https://developer.hashicorp.com/terraform/language/settings/backends/configuration

We can store in consul ==> it is a hashicorp product and stores in the key value pair

What all can be part of backend ?

1. azurerm
2. consul
3. gcs
4. kubernetes
5. local
6. oss
7. pg
8. s3
9. etcd (kubernetes)


Consider s3 as storage (Simple Storage Service)

It is object storage service 

Object Storage 
Block Storage ==> supported by san devices
File Storage ==> 


Media files could also be directly stored onto the s3 bucket
s3 as artifactory can also be used.

s3 is a global service then it should be having a unique name acros the world

we need to get the region for the s3 bucket ==> Why ? ==> S3 is global but the bucket should be present in certain region

we need to maintain the version for the s3 bucket also for mainitainance and tracking purpose

in s3 key is the filename

  backend "s3" {
    bucket = "terraformstatefiles3backend"
    key = "teraform-state-file-backend-s3"
  }

aks us to add a region we can add directly in the backend the region as a field then

  backend "s3" {
    bucket = "terraformstatefiles3backend"
    key = "teraform-state-file-backend-s3"
    region = "us-east-1"
  }

While createing it on the AWS we should consider

to uncheck the  "Block Public Access settings for this bucket"

Enabling the bucket versioning  

Disable the Bucket Key

Fields in the backend "s3" adn their functions 

1 .bucket =>  gives out the names of the bucket
2. key => It is nothing but the file name and state file will be stored in it. Do not need to create it TF will take care to create it.
          eg key = "terraform-state-file-june-23"
3. region ==> The region where the s3 bucket we want to store eg. us-east-1

Why we need to reinitailise ?

As we are storing the data / tfstate file in the backend i.e storing in the s3.
So the statefile which is avilable in the local right now we want to make it avilable in the s3 as well.
So we need to infrom the plugin "terraform provider" that we have made the change of storing the state file in the s3 bucket provided by user in the configuration

For thuis purpose we need to initialise
terrafrom init ==> used to initialise the creation

still not run as it do not have permissiopn ==> What to do ? It needs to assume the role in the backend in the terraform block

As mentioned in the provider.tf file ==> It has first the terraform block and then the aws provider block where we have assumed the role ==> so we need to explicitly add the role_arn field to the terraform block and it will give it the required permission


terraform init ==> create the state file tfstatefile

Try destroying it ? 

terraform destroy it will destroy the gebnerated tfstate file in the s3 bucket also 

terraform state pull
################################################


#####################################Day 15 ##########################################

As we have commented the bucket part in the provider.tf ==> we need to renitialise it using the terraform init
Again applying terraform plan 

We face an error like 

 Reason: Unsetting the previously set backend "s3"
│ 
│ The "backend" is the interface that Terraform uses to
│ store state,
│ perform operations, etc. If this message is showing up,
│ it means that the
│ Terraform configuration you're using is using a custom
│ configuration for
│ the Terraform backend.

What it means is that there was earlier a resource in the aws s3 with some name and as now we are omitting it so we need to reconfigure 

So for the rconfiguration it can be done with the help of ==> terrafform init -reconfigure  or terraform init -migrate-state

Based on use case: 

terrafform init -reconfigure ==> It will completely reconfigure and locally created state file will be considered.
terraform init -migrate-state  ==>  The state from the state file which is availabel in the s3 will be migrated to the local.

If state file resources are available in the different location them yhe migrate state is preffered.
If we want to create everything from the scratch from the new state store we should then consider the reconfigure


In our case we are  migrating the state 
terraform init -migrate-state

If we try to apply now from the vscode terminal then it will be 

teraform apply --auto-approve

It will run in the external terminal but will throw out the error for our local system in vscode

What error does it pass to the local system is 

Error: Error acquiring the state lock
│ 
│ Error message: resource temporarily unavailable
│ Lock Info:
│   ID:        aea5fe89-e8c0-d0bc-f472-1023f59130d4
│   Path:      terraform.tfstate
│   Operation: OperationTypeApply
│   Who:       aditya@aditya-Inspiron-3576
│   Version:   1.4.6
│   Created:   2023-06-21 10:49:29.294483941 +0000 UTC
│   Info:      
│ 
│ 
│ Terraform acquires a state lock to protect the state
│ from being written
│ by multiple users at the same time. Please resolve the
│ issue above and try
│ again. For most commands, you can disable locking with
│ the "-lock=false"
│ flag, but this is not recommended.

NOTE: In TF the same operrtion should not happen at same time from two diferent locations

NOTE:SO till the time terrafrom apply is running on extrnal terminal; it cannot be runned from another location

NOTE: It is used to prevent duplicacy or changes from two different location at the same time

NOTE: In the local state storage it is by default available.

Q. What if I want to initialise whatever in teh state file in localshould be present in the s3 also>
use terraform init -migrate-state



But when we run tghe terraorm apply -migrate-state on boith the terminal now it is running and do not give the error related to state lock ? WHY ?

==> When we have the state file available locally then terraform takes care of locaking the state so that multiple places we cannot perform the operation related to terraform
But when the backend is stored in the Remote then we need to take care of the locking as well.  


So we need the locking mechanism for the state locking ? Is there some method so that from one palce it could be applied ?

We can use the DynamoDB Table for state locking


In S3 backend ==> Dynamo DB State locking

1. Create the DynamoDB Table ==> It is a NO SQL Database. It not only stores the value in structured and unstructured data can completely managed by AWS
table name as "terraformstatetable"

NOTE: A "Partition Key" also must be added by name "LockID" and type should be a "string"


NOTE: We do not need to take care of the integration of the AWS S3 and DynamoDb integration after providing the partition key as LockID

Add it to the backend block in terraform block in the provider.tf

dynamodb_table = "terraformstatetable" ==> dynamodb_table ="name of the file"


  backend "s3" {
    bucket   = "terraformstatefiles3backend"
    key      = "terraform-state-file-june-23"
    region   = "us-east-1"
    role_arn = "arn:aws:iam::972348856143:role/stsassumerole"
    dynamodb_table = "terraformstatetable"
  }

Now try to run ==> But first we need to initialise the terraform file with terraform init -reconfigure

try running it on both the browsers

It naow enables the state lcoaking for us 
-------
Now we are going to move towards the autoscaling group ==> we need to create the insatcnes for webserver and the application server and alos need to create the RDS

WE can create using the EC2 instance and another one using the Auto scaling group
-----
Before than we have "locals"

we can define our local variable in the "locals block"

locals vs variables ??

Local acts as input variable only and based on variables we can create a condition
We can define local variable in the locals block

- In variables we can have the value of particular datatype; but I want some customization it should also have some extra charecters in the end.

eg we want to add the vpc_name with some other info then we need to make use of the locals block.

eg creating locals.tf in which it contains 
locals {
  vpc_name_local = "${var.vpc_name}-lwplabs"
}

In the variable we cannot do such things and so we need to make use of locals we can only make use of default value it will only be taken if values not provided by user.

Scenario: If for 2 AZ we want some condition and for 4 AZ we want another condition then ?

It can be done with the help of this solution 
locals {
  vpc_name_local = "${var.vpc_name}-lwplabs"
  az_required = var.env == "prod" ?  slice(data.aws_availability_zones.this.names, 0, 2) :  slice(data.aws_availability_zones.this.names, 0, 4)
}


Now we want to reflect these changes on our vpc so we need to make changes on network.tf

resource "aws_vpc" "this" {
  cidr_block           = var.cidr_for_vpc
  instance_tenancy     = var.tenancy
  enable_dns_hostnames = var.dns_hostnames_enabled
  enable_dns_support   = var.dns_support_enabled
  tags = {
    Name = var.vpc_name
  }
}

In tags we need to make a change here like 

  tags = {
    Name = local.vpc_name_local
  }

  Remember while retriving the data from local we use local but not locals.

----------------------------------
We will createthe EC2 insatcneresource and another one with the auto scaling

create webserver.tf  => search for aws instance resource terraform,

resource "aws_network_interface" "foo" {
  subnet_id   = "${aws_subnet.my_subnet.id}"
  private_ips = ["172.16.10.100"]

  tags = {
    Name = "primary_network_interface"
  }
}

create the ec2 insatnce using aws console

webserver =>  amazon linux => create a key-pair(ultimate-cicd used here)

  ami   = "ami-022e1a32d3f742bd8"
  instance_type = "t2.micro"
  key_name= "ultimate-cicd-pipeline"

we we are doing is we are having a variable with name web_server

in variable.tf ==> 

variable "web_server_name" {
  type = string
  description ="name of the ec2 instance in which webserver is present"
}

we should take the web server name from the user so add it into tfvars.tf file

web_server_name = "terraform-webserver"

adding it by local we need custom name to it

locals {
  web_server = "${var.web_server_name}-terraform"
} 


To make use of the locals in the webserver.tf file for the web server created by aws instance

tags {
  Name = "lcoal.web_server"
}

In this we also need to provide the subnet id and we do not want to create anything in public but the private so we can make use of subnet_id of private

subnet_id = aws_subnet.private_subnet

But it would not accept it as it would require only string for this so we get

to make use of for loop

subnet_id = [for each_subnet in aws_subnet.private_subnet : each_subnet.id]

But we need to create it only on 1 part so we can get to pick using the "element" function

subnet_id = element([for each_subnet in aws_subnet.private_subnet : each_subnet.id],1)

but we have allowed all the teraffic in our case in our ingress(Inbound Rules) but in real case sceanrio there is not such a case so we need to create a security group also


search aws security group resource terraform

resource "aws_security_group" "allow_tls" {
  name        = "allow_tls"
  description = "Allow TLS inbound traffic"
  vpc_id      = aws_vpc.main.id

  ingress {
    description = "TLS from VPC"
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.main.cidr_block]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_tls"
  }
}

in webserver.tf

We want to mke it open fo the webserver so we need to add the port 80(apache/httpd) in ou ingress Rules

ingress {
    description = "TLS from VPC"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.main.cidr_block]
  }

  We have not creaated the load balancer yet so we should open the cidr block for this only

ingress {
    description = "TLS from VPC"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.this.cidr_block]

  }

Also we need to add nother inbound rrule for SSH purpose so add port 22 for that so we need nother ingress block

ingress {
    description = "TLS from VPC"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.this.cidr_block]
    
  }

  Gebnarally the traffic to the webserver should be connected via the Load balancer and with that we need to connect it to the webserver 

  But can we directly conect to our webserver ?

  No we cannot connect directly to our webserver as its in Private Subnet and we need the help of bastion server for that

  outer people wanting to access our webserver can only access via

  aws_nat_gateway --> loadbalancer ==> bastion host ==> webserver in the private subnets

  IMPOTANT NOTE:: Also the NAT Gateway will block all the reqest coming from outside if there is absence of bastion host as no one can directly aaccess the webserver in the private subnet.

  All the dependencies of the web server will be installed by that.

  Also make a note that from connecting via the Bastion host we need to have
  
  > We need to have the pem key file of the web server instance as it will be required and also

  > To connect it via ssh and pass its Pivate IP address (NOT PUBLIC)

  
egress represnt all the outbound rules and it should be for all the traffic.


  vpc_security_group_ids = [aws_security_group.this].idss

  as we are creating the instances in a VPC we are makiing use of this 

  if not ec2 clasic and the vpc default we can make the use of "security_groups"

  

################################### END DAY 15 ################################################

################################### DAY 16 Create security groups Ingress/inbound rules using the dynamic block and define the variables using the object data type inside define nested ################################################
As we need to have the less repetetion of the code and to make it more usable in this so wht we needed to do is taht we should make use of some reuable componmenet 

Extension needed "GlassIt-VSC" to add: Press ctrl+alt+z to increase the transparency, ctrl+alt+c to decrease.

We make use of  Dynamic Block in it 

1. Dynamic block is supported inside the Data, Povider Block and Provisoner Block
2. It acts as the for loop but it produces nested blocks. Note: For loop cannot be running the nested blocks.

Some formatting in the dynamic block
 It has keywod dynamic "name_of_provider_obhject" aand inside it it has the for-each loop contianing list/map/set
NOTE: Generally we cannot provide the value of list type of value to for_each but it is valid in the case of the dynamic block

Iterator is an optional argument;  if not provided explicitly then the block name is considered as the iterator name

BEST PACTICES for DYNAMIC BLOCK:

1. Not overuse Dynamic blocks
2. Minimise use of Dynamic blocks
3. Not to complex the use of the Dynamic Block with multiple levels of Dynamic Blocks chains


Adding it like in the webserver.tf
  dynamic "ingress" {
    for_each = var.inbound_rules_web // define it in the variables.tf  
  }

  Now as per our discussion lets try to make all the fields like the description and to port from pprt also variables (as they will be changing) so just add it into the variables.tf file

  All the fields needed are having some different dataypes needed like

  {
    port = number
    description = string
    protocol = string
    cidr_block = list(string)
  }

  Do we have any datatype which can enclsoe them all ? ++> Yes, map(object)({})

  so what he datatype would look like is this 

  type = map(object)({
    port = number
    description = string
    protocol = string
    cidr_block = list(string)
  })

  so adding it up on the variable will look like 

  variable "inbound_rules_web" {
  description = "ingress rules for security group of web server"
  type = map(object({
    port = number
    description = string
    protocol = string
    cidr_block = list(string)
  }))   
}

or we could also have the list | map| set in place of the  type = map(object({})) | type = list(object({}))  | type = type = set(object({}))

we can easily make a chaneg like 
  variable "inbound_rules_web" {
  description = "ingress rules for security group of web server"
  type = list(object({
    port = number
    description = string
    protocol = string
    cidr_block = list(string)
  }))   
}

Q. What if we dont know the Datatype which si needed then what can we do here in such case ?
we could make the use of the list(any({})) as the Datatype

Also what could be the default should be added to it and it can be done as follows like in variable.tf

  default = [ {
    port = 22
    description = "This is for the ssh connection here"
    protocol = "tcp"
  }] 

  As we have not implemeneted the load balancer till now so we cannot make use of the cidr_block so we can just generally add it by tghe help of the cidr_block value access with webserver.tf so remove the fiels from the 

    variable "inbound_rules_web" {
      description = "ingress rules for security group of web server"
      type = list(object({
        port = number
        description = string
        protocol = string
        cidr_block = list(string) [Remove this field]
    }))   
}

Q How to add the 2nd block here for 2nd port?
We can ad just like this
  default = [ {
    port = 22
    description = "This is for the ssh connection here"
    protocol = "tcp"
  },
  {
    port = 80
    description = "This is for the webshoting connection"
    protocol = "tcp"
  } ]

  Now getting on with the dynamioc block and making it work like
  
  after the for_each loop add the content{} block and just add all the field details like

    content {
      description = var.inbound_rules_web.description
      from_port   = var.inbound_rules_web.port
      to_port     = var.inbound_rules_web.port
      protocol    = var.inbound_rules_web.protocol
      cidr_blocks = [aws_vpc.this.cidr_block]
    }


    But will this works while iterating through the port values ??
    It will pop  error as it would be confused which description to print 
    for this var.inbound_rules_web.description

    So making the use of the fo_each = var.inbound_rules_web

    from_port   = each.value.port
    to_port   = each.value.port
    protocol    = each.value.protocol
    
    But adding description as description = each.value.description
    But it will not work here beccause error pops

    │ A reference to "each.value" has been used in a context in which it is unavailable, such as when the configuration no longer contains the value in its
│   "for_each" expression. Remove this reference to each.value in your configuration to work around this error.

The for_each is stored as a whole and we cannot just log into it simply by providing the each.value.description

 what to do in such case just make ue of the iterators in the dynamic block

 ==> So the name of the block can act as the iterator variable by default so we need it add it as like that

description = each.value.description  ==> description = ingress.value.description

and so on 
      description = ingress.value.description
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = ingress.value.protocol



W can use the dynamic server here in egreess as well (but not done)




################################### END DAY 16 ################################################


######################################## Day 17 
GitHub Action:
Introduction Yml file
Create simple workflow
Create Window GitHub runner
########################################


YAML : Yert another markupo langguage

It can support list, map datatypes.

to define key value pair in yaml ==> name: Ravi

Initilaise yml file by adding "---" 3 hyphens

github actions uses the yml to write the pipelines
----------
To define map / dictionary in yml we get

address:     ==>  another way company_address: {plot_no: 1, street: kormangla, city: blr}
  h.no : 1087   
  street: sakkardara
  city: hyd
  pin:: 12344

----------

list or array in yml fil  

skills:    ==> similar to this [tf, k8, devops, ansible, {"softskill": "team player"}]
  - tf 
  - k8
  - devops
  - ansible
  - softskill: team player

another way of writing a list

previous_employer: ["ABC", "PQE"]



----------------------

how to preserve the comments 

feedback_from_manager: |      ===> | allows us to split it in the multiple lines
  he is a great guy
  he is a great guy
  he is a great guy

yml is important from ansible and kubernetes perspective


-------------------------------------


%%%%%%%%%%%%%%%%%%%%%%%% GitHub ACtions %%%%%%%%%%%%%%%%%%%%%%%%%
Want to create action goto repo ==> Actions Tab ==> Simple Workflow

1. Here wer can find various templates for already written yml under the Deployment, Security, CI, Automation, Pages

WE do not need to start from screatch there are lots of templates for the deployments and various purpose of workflows

Jenkinss dominace has fallen as our source repo has the functionality to create the pipeline, configure plugins and performing different phases of deployment


We ar egonna use Simple Workflow==> It will get us in the folder .github/workflows/blank.yml 

With this code 

# This is a basic workflow to help you get started with Actions

name: CI

# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the "main" branch
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v3

      # Runs a single command using the runners shell
      - name: Run a one-line script
        run: echo Hello, world!

      # Runs a set of commands using the runners shell
      - name: Run a multi-line script
        run: |
          echo Add other actions to build,
          echo test, and deploy your project.




name: CI ==> its name

# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the "main" branch ==> This will trigger when we push or pull from the main it will automatically trigger
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]


workflow_dispatch: It means if we want to run the workflow manually then we need to make use of this 

Why we need workflow_dispacth ?

Some time it may happen that thee is nothing to commit but we are facing some errors so to manually debug that we should make a provision of the workflow_dispacth(for manual intervention; it canot be create  on the run)

jobs: The workflow can contain different jobs but here it contains only build job for now as its a simple workflow

build: 
  runs-on: ubuntu-latest  // This is called as runnner

  Runners can be found in our repo like ==> Settings ==> side nav ==> Actions ==> Runner

  Runner helps in the whatever commands we give via the Workflow where would it execute, it should be some container or server or container consiting of server(discuss in docker)


  So we can provide the runner by 2 ways profoundly

  1. Github Hosted Runner: It is Completely managed by github eg: windows-latest, ubuntu-20.04, macos-11
  2. Self Hosted Runner


  Whenever an event happens the jobs will run as defined in the workflow; and it would be running on the container or server on container which is provided by the runner. 



  Under taht it will have steps like

  # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v3

      # Runs a single command using the runners shell
      - name: Run a one-line script
        run: echo Hello, world!

      # Runs a set of commands using the runners shell
      - name: Run a multi-line script
        run: |
          echo Add other actions to build,
          echo test, and deploy your project.


  steps: Now if we want to execute any steps on taht we need to checkout(visit) our repository and it will checkout at $GITHUB_WORKSPACE repo (aka our current repo)

   - uses: actions/checkout@v3

   so all the stuff will be stored under the actions/checkout@v3 [actions will be storesd in some location under the runner]  



   -----------------------------------

   let us go to the runner follow tis path   [Runners can be found in our repo like ==> Settings ==> side nav ==> Actions ==> Runner]
    ==> add the linux runenr (as our current os is ubuntu)   
    ==> Run all the commands below (Steps may change but adding it for reference)



    Steps to follow are below

    ^^^^^Download^^^^^^^^^
    # Create a folder
    $ mkdir actions-runner && cd actions-runner
    
    # Download the latest runner package
    $ curl -o actions-runner-linux-x64-2.305.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.305.0/actions-runner-linux-x64-2.305.0.tar.gz
    
    # Optional: Validate the hash
    $ echo "737bdcef6287a11672d6a5a752d70a7c96b4934de512b7eb283be6f51a563f2f  actions-runner-linux-x64-2.305.0.tar.gz" | shasum -a 256 -c
    
    # Extract the installer
    $ tar xzf ./actions-runner-linux-x64-2.305.0.tar.gz

   ^^^^^^^Configure^^^^^^^^^^^^^^

   ./config.sh --url https://github.com/adityadhopade/3-tier-app --token ALRGQDALITH3NQPABKKKHGDETV5XO

   pops enter the name of the runner group : we do not have any runer group for now so just press enter

   enetr name of the runner ==> aditya-runner

   This runner will have the following labels: 'self-hosted', 'Linux', 'X64' 
Enter any additional labels (ex. label-1,label-2):

  so it will add those labels like self hosted , linux and the x64 and if we wanted to add more specific label we can add it now ==> do not need so press enter

  √ Runner successfully added
√ Runner connection is good

# Runner settings

Enter name of work folder: [press Enter for _work] 

Here it is the folder where the source code will be checkout from the repo and stored into github runner => press enter


In windows we may get pop upl ike 
Would you like to run the runner as a service  

If yes then it ruins a service
If no then we would have to run this with the help of ./run.cmd


Now just run this command

   ./run.sh ==> It will start our runner

But it will only get triggered when there is push or pull in the main branch so whaat we need to do is 

just change the 

on : ubuntu-latest  ==> to our label that we created i.e. either of the self-hosted, linux or X64


Also before commiting change the name of the yml file as simple.yml

commit it ==> so it will have now the folder structure .github/workflows/simple.yml


Now we can see some chanegs in the Actions tab ==> Insider that recent commit name ==> build

-------------------------------------

Terraform refresh ==> It is depricated

We do not need to make use of it ==> A s when we do the terraform apply it automatically refreshes the state file befor applying it

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



########################################  END Day 17 ########################################

########################################   Day 18 ########################################

Created a Bastion host block to connect to a private network.
-Created web.sh block for shell scripting for web server configuration
-Created Local provisioner in bastion host block to automate the public access by using ssh

--------------------
As far as now we  need to cretae a web application on the web server.

It can be done by the 3 ways they are as follows: 

1. AWS provides a feature in it so that we can add all our commands on the user data block.
We can see it 2hile creating the EC2 instacne ==> Advance Details ==> User Data Text Area here we can add all our commands /file and it would be encrypted in base64


2. we can pass the user_data a shell script file it will contain the different part of it.
so we need to create a user_data.sh file and it will have akll the steps to launch a server

#! /bin/bash    

#! ==> Indicates if you are giving a script file in linux in that we provide with #! and path is provided
Here as we are wanting to use the nash script we need to provide their path where the binary is located os we needed to add it as the /bin/bash

/bin/bash ==> Location of bash

eg for python it would act as ==> #! /bin/python3


As here we are going to use the Amazon linux it comes under the fedora distribution so we needed to make use of the yum package installler.

Now we need to install a web server take Apache but in the fedora distribution apache comes with the package "httpd"

So in case of fedora its ==>  yum install httpd
In the case of Ubuntu it is ==> apt install apache2 (latest version)



Now we need to enable the service by ==> sudo systemctl enable httpd
Now we need to start the service by ==> sudo systemctl start httpd

Then to check wheather our web server is working or not we can make use of the echo command with the tee command

sudo echo '<h1>Hey there! Welcome to the web server</h1>' | sudo tee /var/html/index.html 

tee command reads the standard input and writes it to both the standard output and one or more files. The command is named after the T-splitter used in plumbing. It basically breaks the output of a program so that it can be both displayed and saved in a file. It does both the tasks simultaneously, copies the result into the specified files or variables and also display the result.

sudo echo '<h1>Hey there! Welcome to the web server</h1>' 

and add to the | sudo tee /var/html/index.html 


so how can we add it into the webserver.tf under the field user_data

We can add it as following ]

user_data {
// All the script we added in the user_data.sh but it leads to duplication and will not look as good practice

}

We need to make use of the file function in terraform

file function reaads the content of the given path and returns them as a string

[Complex topic can be easily understand by module]

Need to make use of it like file("${path.module}/hello.txt")

${path.module} ==> To explain this 

Whatever we have written till now we can make it as the module and adding a main.tf file which consumes the module then the main.tf will acts as the root folder now. and the module will become child

root => main.tf
child ==> all the rest files (modules)

And if we are givig the ${path.module} to the main.tf ==> It means we are referencing to the same current location denoted by .

If we are providing in module in some file by ${path.module} it will give us the relative path betwen the module and the main.tf

so now we can put it as the  user_data = file("${path.module}/user_data.sh")
--------
After doing this we need to verify taht can we directly connect to the webserver present in the private subnet ?

We can connect via the Bastion Host but not directly


-----
Also for the key field check if the key is available or not (usig the already created ultimate-cicd-pipeline.pem)

-----

For the lock happenming on the we can terraform plan make the use of

1. creating a new bucket ==> terraformstatefiles3backendpart2
2. adding it to our provider.tf file 
3. deleting the terafrom provider folder
4. run terraform init
-------------------

bastion_host.tf

resource "aws_instance" "bastion" {
  ami           = "ami-022e1a32d3f742bd8"
  instance_type = "t2.micro"
  key_name      = "ultimate-cicd-pair"
  subnet_id     = element([for each_subnet in aws_subnet.public_subnet : each_subnet.id], 0)

  tags = {
    Name = local.bastion_host
  }

  vpc_security_group_ids = [aws_security_group.bastion_host.id]
  user_data              = file("${path.module}/user_data.sh")
}

resource "aws_security_group" "bastion_host" {
  name        = "allow_bastion_host"
  description = "Allow ssh into the private subnet resources using this"
  vpc_id      = aws_vpc.this.id

  ingress {
    description = "Allow the ssh traffic to private subnet bastion host from private subnet"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_bastion_host_security"
  }
}



locals.tf

locals {
  vpc_name_local = "${var.vpc_name}-lwplabs"
  web_server     = "${var.web_server_name}-terraform"
  bastion_host   = "${var.vpc_name}-bastion-host"
}





Text Written

-----------------------------------------------

Can anybody outside can connect with application server which is present on the private subnet directly ??

No they cannot they can only connect via the help of the bastion host even if they try to connect the NAT Gateway obstructs the user and do not allow to access the application server.

----------------------------------------------------

What is the need of the bastion Host(Jump Servers) ?

in the running server there some error may be coming and we want to verify eg suppose there is a ddos attack, we want to trash the ips nad want to check the error logs, so we need them to trouble shoot at realtime by using the jumop server or bastion host
---------------
Add a bastion_host.tf file here
-------------------
here copy the template code from the webserver.tf file and make some changes as follows

resource "aws_instance" "bastion" {
  ami           = "ami-022e1a32d3f742bd8"
  instance_type = "t2.micro"
  key_name      = "ultimate-cicd-pair"
  subnet_id     = element([for each_subnet in aws_subnet.public_subnet : each_subnet.id], 1)

  tags = {
    Name = local.bastion_host
  }

  vpc_security_group_ids = [aws_security_group.this.id]
  user_data              = file("${path.module}/user_data.sh")
}

resource "aws_security_group" "this" {
  name        = "allow_webserver"
  description = "Allow web traffic"
  vpc_id      = aws_vpc.this.id
  dynamic "ingress" {
    for_each = var.inbound_rules_web
    content {
      description = ingress.value.description
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = ingress.value.protocol
      cidr_blocks = [aws_vpc.this.cidr_block]
    }
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_webserver"
  }
}

Here we also needed to add the custiom security group as we do want the restricted accesss to it

resource "aws_security_group" "bastion_host" {
  name        = "allow_bastion_host"
  description = "Allow ssh into the private subnet resources using this"
  vpc_id      = aws_vpc.this.id

  ingress {  // Do not need the dynamic block here as we only neeed one port here for ssh (22)
    description = "Allow the ssh traffic to private subnet bastion host from private subnet"
    from_port = 22
    to_port = 22
    cidr_blocks = ["0.0.0.0/0"]  // to allow all traffic
  }

} 


here are all the cahnegs that we have made to the bastion_host.tf file 

resource "aws_instance" "bastion" {
  ami           = "ami-022e1a32d3f742bd8"
  instance_type = "t2.micro"
  key_name      = "ultimate-cicd-pair"
  subnet_id     = element([for each_subnet in aws_subnet.public_subnet : each_subnet.id], 1)

  tags = {
    Name = local.bastion_host
  }

  vpc_security_group_ids = [aws_security_group.bastion_host.id]
  user_data              = file("${path.module}/user_data.sh")
}

resource "aws_security_group" "bastion_host" {
  name        = "allow_bastion_host"
  description = "Allow ssh into the private subnet resources using this"
  vpc_id      = aws_vpc.this.id

  ingress {
    description = "Allow the ssh traffic to private subnet bastion host from private subnet"
    from_port   = 22
    to_port     = 22
    cidr_blocks = ["0.0.0.0/0"]
    protocol = "tcp"
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_bastion_host_security"
  }
}

so what we need is to to try to access the bastion host are we able to reach it via ssh.

For that we need to do is 

ssh -i <location of the pem file> ec2-user@<public_ip_ofec2_instance>

in our case its ssh -i ~/lwplabs/access_keys/key_pairs/ultimate-cicd-pair.pem ec2-user@

ssh -i ~/lwplabs/access_keys/key_pairs/ultimate-cicd-pair.pem ec2-user@ec2-44-204-57-128.compute-1.amazonaws.com


NOTE: We need to modify our public and the private subnets and add it a field like 
map_public_ip_on_launch= true

and ensure that the coreect key is passed to basttion host and the web server

------------------------------
can we connect directly to the web server via bastion host ?

Yes we need to pass the key to it first

Firstly try logging in bastion host ssh -i ~/Downloads/new-ultimate-cicd.pem ec2-user@3.236.29.67

Q .Then can we connect application server via bastion host using public ip ?
NO we cannot we need to make use of the private ip address

ssh 10.0.0.46 (No need to pass the user as its already considered)

Q. but it will also not connect to application server now WHY ?
It denies it because 
Because we need key file(new-ulimate-cicd.pem) here 

-----------------------------------------

How can we pass the public key to the server ?

We can do it using the scp like below

scp -i  ~/Downloads/new-ultimate-cicd.pem ~/Downloads/new-ultimate-cicd.pem ec2-user@3.236.29.67

This will copy it but will not work below

scp -i  ~/Downloads/new-ultimate-cicd.pem ~/Downloads/new-ultimate-cicd.pem ec2-user@3.236.29.67:~

chmod 400 new-ultimate-cicd.pem
--------------
As we have the pem file in the bastion host then can we connect to the app server ?

ssh -i new-ultimate-cicd.pem ec2-user@10.0.0.46

we can also do it like ssh -i new-ultimate-cicd.pem 10.0.0.46 (As the user for both the bastion host and the web server is the same)

Now we need to make sure that we are connecting to the web server 

make use of the => curl localhost

not works and not connects

As we have made a mistake in the user.sh file in the command sudo teee ==> should be sudo tee ; so we need to make sure that the process id in the background gets killed and start from scratch

use => sudo rm /var/cache/dnf/metadata_lock.pid

----------------

Now we need to make sure that our Private route table consists of the right entries in it we needed to make sure of there is some route fo the NAT Gateway

As there is no route added at the private route table so we need to have in the network.tf

route {
  cidr_block = "0.0.0.0/0" ==> it should allow all traffic as it internet facing id
  nat_gateway_id = aws_nat_gateway.this.id => to fetch the nat gateway id
}


Then again ty connecting to the web server

ssh -i ~/Downloads/new-ultimate-cicd.pem ec2-user@publicip-address
ssh -i new-ultimate-cicd.pem <private ip addreess of app>
curl localhost
<Addd commands from .sh file>
sudo yum update -y
sudo yum install httpd 
sudo systemctl enable httpd
sudo systemctl start httpd

sudo systemctl status httpd

But we need to make sure we are automating this process taht we have followed through the above commands

So we need to make sure that we are using the "Provisoner" in Terraform

Proviosiners: Provisioners are used to execute scripts on a local or remote machine as part of resource creation or destruction. Provisioners can be used to bootstrap a resource, cleanup before destroy, run configuration management, etc.

Whatever we write in the proviosners will be executed as soon as the resources is created.

So we will be using the proviosner in the bastion_host.tf 

NOTE: We should be using the proviosner as the last resort as it make s the terraform configuration complex.(Not recommended to use it)

provisioner "local-exec" {
  command = "scp -i  ~/Downloads/new-ultimate-cicd.pem ~/Downloads/new-ultimate-cicd.pem ec2-user@3.236.29.67:~"
}

But the public ip can change if instances are stopped and staretd again then what we needed to do ?

provisioner "local-exec" {
  command = "scp -i  ~/Downloads/new-ultimate-cicd.pem ~/Downloads/new-ultimate-cicd.pem ec2-user@${self.public_ip}:~"
}


If we do not need a pop up to add the fingerprint from the next time then we can just modify our command as 

  provisioner "local-exec" {
    command = "scp -o StrictHostKeyChecking=no -i  ~/Downloads/new-ultimate-cicd.pem ~/Downloads/new-ultimate-cicd.pem ec2-user@${self.public_ip}:~"
  }

Also we need to make sure that the coreect permisiions are passed to the pem key file s so we can make use of the resources "remote-exec" 

ssh -o StrictHostKeyChecking=no -i new-ultimate-cicd.pem 10.0.0.46

remote-exec to be used when once the resource is created and we want to make use of some commands then the remote-exec comes in handy eg changing the permissions for the pem file.

we can also make use of ssh -o StrictHostKeyChecking=no -i new-ultimate-cicd.pem 10.0.0.46 while logging in the app server from the bastion host



------------------------

But as we are making chnages in the provisoiner commands will it also make a change when done terraform apply ?
Ideally it should but it will not do it beacause provuisoner only runs when you apply it i.e. during the 1st terraform init only

Cant we make sure that it will run the content of the proviosioner everytime whenevr we make he changes ?
Yes there is making use of Null Resource

Null Resource ==> DAY 19


How to use the reverse search in linux ?
ctrl + R => search for the term 


########################################  END Day 18 ########################################

########################################  Day 19 Null Reosurces ########################################

There are multiple kind of supporting providers which could perform a following task like eg To generate a random number there is resource for it random provider (acts like built in functions for the terraform configuration for enchancing code and making it efficient)

Random proviosioner can be used to make the passwords like for the auto generated passwords.


Null Reource ==> 

we have to make  sure that they need to be added provider into the resource required_providers of provider.tf as 

  null = {
    source  = "hashicorp/null"
    version = "3.2.1"
  }

NOTE: It is not recommended to have a ll the terraform plugins in the same configuration here. As if one provider breaks then it will compromise the other resources adn as the tf state file acts as the single source of truth then it will definately be a hasle to resolve.

---------------
for the above reason create the null_Resource.tf file and add the sample text from the documentation of null_Resource

resource "null_resource" "provisoner" {
  # Changes to any instance of the cluster requires re-provisioning
  triggers = {  # when do you want to run this resource
    cluster_instance_ids = join(",", aws_instance.cluster.*.id)
  }

  # Bootstrap script can run on any instance of the cluster
  # So we just choose the first in this case
  connection {
    host = element(aws_instance.cluster.*.public_ip, 0)
  }

  provisioner "remote-exec" {
    # Bootstrap script called with private_ip of each node in the cluster
    inline = [
      "bootstrap-cluster.sh ${join(" ",
      aws_instance.cluster.*.private_ip)}",
    ]
  }
}

we want the trigger to run at at each run of terraform apply 
triggers {
  always_run = timestamp()  // always run for each timestamp as when we run the terraform apply then it will have a different timestamp at each terraform apply
}

also copy the provisoner here so we can take a look at the difference of what it makes

  provisioner "local-exec" {
    command = "scp -o StrictHostKeyChecking=no -i  ~/Downloads/new-ultimate-cicd.pem ~/Downloads/new-ultimate-cicd.pem ec2-user@${self.public_ip}:~"
  }

But it will not run beacuse it pops the error and it is beacuse it cannot get the public_ip of the address in there. So we need to pass the full path to the resource like

  provisioner "local-exec" {
    command = "scp -o StrictHostKeyChecking=no -i  ~/Downloads/new-ultimate-cicd.pem ~/Downloads/new-ultimate-cicd.pem ec2-user@${aws_instance.bastion.public_ip}:~"
  }

It will work 

Also we wanted to have make changes to the permission of the key file ==>  chmod 400 <pemkey> . It can be done with the help of the remote exec

so we can define the remote_exec proviosner as following in null_resource.tf

provisioner "remote-exec" {
  inline = [
    "chmod 400 test.pem"
  ]
}

But for that to work we need to establish a "connection block" as it provides that how we want to connect to the remote server using this.

connection {
  host = aws_instance.bastion.public_ip # as we need to pass the host public ip address(bastion in our case) and it takes it using this 
  type = ssh
  user = "ec2-user"
  private-key = file("${path.module}/../../Downloads/new-ultimate-cicd.pem")
}
  we are using the private key beacuse as its not available locally n our codebase so to make a reference to the file we can make use of it.
  file function what it does it whatever is in the file it considers it in the form of string
---------------------------------------

NOTE: In the initial connection which are provided via the providers it may give the ssh connection timeout  

re run the terraform in taht case 

--------------
we can ad the depends_on block in our case so taht if the resource is completely up and running then only it will run the null resource in the null_resource.tf

depends_on = [aws_instance.bastion]

--------------------------

We can alos add another type of the provisoner like file provisioner

provisioner "file" {
  source = "~/Downloads/new-ultimate-cicd.pem" # where do you want to fetch the file from
  destination = "/home/ec2-user/mykey" # to what destination we want the file to be taken to

  Also we could add a field like "content" inplace of the source interanlly runs same scp commands to copy the conent from source to destination

  content = file("${path.module}/../../Downloads/new-ultimate-cicd.pem")
}


So this is all about the null resource and the provisoners
---------------------------

We wanted to create our own key file instead of the key-file(.pem file) provided by the aws can it be done ?

Yes it can be done with the help of the git ==. using the ssh keygen

Using the ssh keygen it establsihes both the Public and the Private key 

Advantages of using the public key i that even if anybody got access to your public key they cannot access our resources directly we need to provide the private key with it always.

We would provide the public key to the aws that aws would use for the instance creation and private key will kept to ourselves


Also for the credentails management we can use the Vault as the storage for it.


But for the above we can manage it by doing 
ssh-keygen

add the cureent folder path as we want to generate the file into our local folder first and give it some name as "keypair"

Then it will generate the keypair.pub (public key) and keypair (private key) & search for the "aws_key_pair in terraform" and copy this and add a file named as key_file.tf

resource "aws_key_pair" "deployer" {
  key_name   = "deployer-key"
  public_key = "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD3F6tyPEFEzV0LX3X8BsXdMsQz1x2cEikKDEY0aIj41qgxMCP/iteneqXSIFZBp5vizPvaoIR3Um9xK7PGoW8giupGn+EPuxIA4cDM4vzOqOkiMPhz5XK0whEjkVzTo4+S0puvDZuwIsdiW9mxhJc7tgBNL0cYlWSYVkz4G/fslNfRPW5mYAM49f4fhtxPb5ok4Q2Lg9dPKVHO/Bgeu5woMc7RY0p1ej6D4CKFE6lymSDJpW0YHX/wqE9+cfEauh7xZcG0q9t2ta6F6fmX0agvpFyZo8aFbXeUBr7osSCJNgvavWbM/06niWrOvYX2xwWdhXmXSrbX8ZbabVohBK41 email@example.com"
}


But after adding taht we need to make sure taht we are adding it into our aws or terraform script we can do it by replacing key name in the web_server.tf

key_name = var.key_name

--------------------------

Also we needed to make sure that we are replacing the earlier generated private key with the newly created private key
---------------------------------
There is one another way to configure our server

1st way: pass the user_data vai the user_data.sh file.
2nd way: Creating custom AMI resource and from that creating the web resources.(used in Realtime)

--------------------------------------------

Creating Custom AMI ==> They are genreally used as best practice and the industry standard based on guard rails, different security measeures and muliple things.

Eg==> if we want to use the resource conatining the jenkins and ansible the most trivial way would be just creating instances and then adding the setup for those applcations but it will take lots of time and efforts. 

So it is better to launch the Custom images AMI's and these prebuild AMI's are consisting of all the standards available as defined by the organizations so such images are called as the "Golden Images"

Golden Images = Vanilla Images (Pre existing images) + install requirements and Security measures, standards of organizations

In the AWS it can be build with the help of the "EC2 Image Builder" but it is validfor the AWS only
But with the help of terraform we can build it for the multiple clouds as its "Cloud Agnostic". with the help of "Packer"

Packer => product of Hashicorp used for the iamge building 


########################################  END Day 19 ########################################

########################################  Day 20 Packer ########################################

Packer: So packer helps us to create the golden images which are nothing but the AMI images with preinstalled conifgurations and the best security practices of Orgamziations

----------------------
To implement the packer in that we need to have the image builder folder

We would need to configure the provider plugin like "provider.pkr.hcl" 
Here the .pkr.hcl acts as an extension for the packer


take the sample packer template from the https://developer.hashicorp.com/packer/plugins/builders/amazon  => Amazon EC2 => Builders ==> Overview

packer {
  required_plugins {
    amazon = {
      version = ">= 1.2.6"
      source = "github.com/hashicorp/amazon"
    }
  }
}

Then we will be writing our file for the image creation. like => "ami.pkr.hcl"

Now we can have different typoe of storages for the packer AMI's 

1. amazon-ebs (elastic block storage) It is persisten volumne
2. amazon-insatnce (Its an ephermaeral storage; it gets deletedas soon as instance gets terminated)
3. amazon-chroot Create EBS-backed AMIs from an existing EC2 instance by mounting the root device and using a Chroot environment to provision that device [Quite Advance but the fastest way to build an AMI since no new EC2 instance needs to be launched.]
4. amazon-ebssurrogate Create EBS -backed AMIs from scratch. Works similarly to the chroot builder but does not require running in AWS. This is an advanced builder and should not be used by newcomers.

The packer and terraform shares the same language HCL (HashiCorp Language).

In packer we can write both in the HCL and json as they both are supported here.

---------------------------------------------------

Copy the basic sample code from this link https://developer.hashicorp.com/packer/plugins/builders/amazon/ebs#basic-example and add it to the ami.pkr.hcl

source "amazon-ebs" "basic-example" {
  access_key = var.aws_access_key
  secret_key =  var.aws_secret_key
  region =  "us-east-1"
  source_ami =  "ami-fce3c696"
  instance_type =  "t2.micro"
  ssh_username =  "ubuntu"
  ami_name =  "packer_AWS {{timestamp}}"
}

build {
  sources = [
    "source.amazon-ebs.basic-example"
  ]
}


Just like the datasource in the terraform we have here the source block 

source block contains the base image taht we will be using

---------------

source "amazon-ebs" "basic-example" {
  access_key = var.aws_access_key
  secret_key =  var.aws_secret_key
  region =  "us-east-1"
  source_ami =  "ami-fce3c696"
  instance_type =  "t2.micro"
  ssh_username =  "ubuntu"
  ami_name =  "packer_AWS {{timestamp}}"
}

In the source block we have the access key and secret key but we can also access the user by using the assume role in the ami.pkr.hcl file

so we can add it as follows

source "amazon-ebs" "basic-example" {
  assume_role{
    role_arn     = "arn:aws:iam::869190274350:role/stsassumerole"
  }
  region =  "us-east-1"
  source_ami =  "ami-fce3c696"
  instance_type =  "t2.micro"
  ssh_username =  "ubuntu"
  ami_name =  "packer_AWS {{timestamp}}"
}

We can change the other fields as follows like 

  region =  var.region
  source_ami =  var.image_id
  instance_type =  var.instance_type
  ssh_username =  var.ssh_user_name
  ami_name = local.image_name

  add the local block in the same context as 

  locals{
    image_name = ${var.app_name}-packer"
  }

------------------------------------
Add another file for the variables taht we are using here. variables.pkr.hcl

variable region{
    description = "Provides the AWS Region"
    type= "string"
    default = "us-east-1"
}


variable image_id{
    description= "Provides the AWS AMI id specified in region"
    type= "string"
    default = ""
}

We can also make use of the data_source here in packer to make it more dynamic ==> Add a file "data.pkr.hcl" for that and copy the snippet from the https://developer.hashicorp.com/packer/plugins/datasources/amazon/ami

data "amazon-ami" "basic-example" {
    filters = {
        virtualization-type = "hvm"
        name = "ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*"
        root-device-type = "ebs"
    }
    owners = ["099720109477"]
    most_recent = true
}


For now what we want is taht our filter should act and work accordingly 

so we need to fetch the AMI ids from the  amazon linux instances(As here we want to make the use of those) while using the launch instances

data "amazon-ami" "basic-example" {
    filters = {
        virtualization-type = "hvm"
        name = "al2023-ami-*"
        root-device-type = "ebs"
    }
    owners = ["137112412989"]
    most_recent = true
}


Now there is a process to apply the right filter for the  ami id ==>

Steps:  Go to the Images(Side nav) ==> AMI => In dropdown select filetr Public Images ==> Search for the ami-05548f9cecf47b442(copied AMI ID)  => apply filter we wil get the AMI name

Same in taht filter we will also get the owner name here ==> add it in this field owners = ["137112412989"]

data "amazon-ami" "basic-example" {
    filters = {
        virtualization-type = "hvm"
        name = "al2023-ami-*"
        root-device-type = "ebs"
    }
    owners = ["137112412989"]
    most_recent = true
}

So now the source_ami can be calculated like the =>  source_ami =  var.image_id  ===> source_ami = data.amazon-ami.this.id


So as we are making use of the data source we do not need the varaible for the image_id so just remove it from there variable.pkr.hcl

variable image_id{
    description= "Provides the AWS AMI id specified in region"
    type= "string"
    default = ""
}

Add all the rest variables like 

variable instance_type{
    description = "instacne type of launching AMI"
    type = string
    default = "t2.micro"
}

variable ssh_user_name{
    description = "ssh user name for instance"
    type = string
    default = "ec2-user"
}
variable app_name{
    description = "app name for web server image"
    type = string
    default = "web-server-image"
}


The build block in the ami.pkr.hcl it is used to build the image (We can have multiple sources and the multiple build block)

build {
  sources = [
    "source.amazon-ebs.basic-example"
  ]
}
eg. suppose we want to build 2 golden images i.e. for amazon linux and ubuntu then we we can add source block for both and then write it in the build block also in the sources

Now in our case there is only one image we want to build so we can give it a name as "this"

build {
  sources = [
    "source.amazon-ebs.this"
  ]
}

Also we need to make use of the proviosner here known as the packer provisioner same as that in our teraform

so refer the docs here as mentioned https://developer.hashicorp.com/packer/docs/provisioners/shell

using the shell proviosioner in packer ==> Here we can execute the shell script ; it acts similar to that of the "remote-exec"

provisioner "shell" {
    inline = ["echo foo"]
}


NOTE: We are here adding the provisoner shell so as we can add our webapp server in it.

sudo yum update -y && sudo yum install httpd -y
sudo systemctl enable httpd
sudo systemctl start httpd
sudo echo '<h1>Hey there! Welcome to the web server</h1>' | sudo tee /var/html/index.html 

Copy all the commands from the user_data.sh file and add it in the provisioner shell under the build block

  provisioner "shell" {
    inline = [
        "sudo yum update -y && sudo yum install httpd -y",
        "sudo systemctl enable httpd",
        "sudo systemctl start httpd",
        "sudo echo '<h1>Hey there! Welcome to the web server</h1>' | sudo tee /var/html/index.html" 
    ]
  }

---------------  

Now to initialize the packer it can be done with the following as 

packer init ./image-builder/  [as our flder is child folder of the main folder]

Packer Installation: First we need to install packer with the steps below

curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
sudo apt-get update && sudo apt-get install packer

----------------------

To initialise the packer ==> packer init ./image-builder/ (If packer folder is child folder) else ==> packer init .

To validate the packer ==> packer validate ./image-builder/ (If packer folder is child folder) else ==> packer validate .

To format the packer ==> packer fmt ./image-builder/ (If packer folder is child folder) else ==> packer fmt .

To build the packer ==> packer build ./image-builder/ (If packer folder is child folder) else ==> packer build .

Here in the variable.pkr.hcl we have the instance_type default decalred as following 

variable instance_type{
    description = "instacne type of launching AMI"
    type = string
    default = "t2.micro"
}

but what if we remove the default field then how can we add the "t2.micro" can be done in multiple ways they are 

1. Adding as the env variables
2. passing with the commands as --var=value_of_var
3. pasing in as var-file 

They are same as tjat of we have in the terrafform

We can use here env variable way like as below

export PKR_VAR_instance_type="t2.micro"
-----------------------------------------

you will eventually face this error 

$ packer build ./Image_builder/
Error: Datasource.Execute failed: Error querying AMI: UnauthorizedOperation: You are not authorized to perform this operation.

NOTE : So its a bug even after provideding the stsassumerole the policy it is not providing the full access to the EC2 user so we needed to provide explicit permission of ec2fullaccess to it.

stsuser should be added with the ec2-full acceess 

----------------------------------------------------

Now check for it in the ec2 ==> AMI => owned by me in the us-east-1 region (as provided by us) => we will see our ami running there

----------------------------------------------------

Now we would be using this golden image for loading our webserver on it and use it 

so we need to define data source it in the data.tf file ==> search data source ami terraform

search for it then we get 

data "aws_ami" "example" {
  most_recent      = true
  owners           = ["self"] # as currently we are only using this images

  filter {
    name   = "name" # name of that particular image
    values = ["*-packer"] # what needs to be conisdered as a filter in our case ending wth packer
  }

  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
}

come to the webserver.tf and comment the user_data field in it we get 

  # user_data              = file("${path.module}/user_data.sh")

Instead we will be referencing it as the 

ami = aws_ami.this.id and as its from the data source it will be like => data.aws_ami.this.id


Also plan and run the teraform it will pop and error for us as we have not passed our private key named "keypair" to the bastion host and the web server

What we will do for now is that we will manually add the pem file using the scp commands like 

scp -i keypair keypair ec2-user@18.233.200.55:~ 

ssh -i keypair ec2-user@<private-ip>

localhost
------------------------------
In the packer there is not a file like terraform.tfvars as this automatically gets loaded when there is no other file containing variables but in the case of the packer it does not exist and we need to provide the name explcittly as "variable.pkrvars.hcl" and if used the "auto" option it will directly use it as  "variables.auto.pkrvars.hcl"

--------------------

########################################  END Day 20 ########################################

########################################  Day 21 new github workflow (github actions) for our ami resources in simple.yml########################################

We would erase all the contents of the simple.yml file present earlier and add the following conetent in it

# give out the name of the "github action pipeline name" what we want it to be 
name: Provionong of threee tier architecture using Terraform

# When to run the pipeline
on: 
  #to manually run the workfloe use workflow dispatch
  workflow_dispatch: 
  # apart from that we need to run the workflow on the push so we are providing the branches explicitly here(i.e. when pushed on the branch it should trigger)
  push:
    branches: [main]
  #if we need to invoker the workflow on the pull requests also then we can add it (or we can add it for the specific branch also like added above)  
  pull-request:   

# What jobs to run
jobs: 
  #name of the job
  provison-three-tier: 
    # on which runner it should run eg: self hosted runner, github hosted runners (for linux=> ubuntu-latest , windows => windows-latest, mac-os => macos-latest)
    runs-on: ubuntu-latest
    # In a job there should always be the some serial steps to execute
    steps:
      # while the first should always be checking out the scm tool
      - name: checkout
        #it is prebuilted and we can directly use it as is  ( same as plugins in jenkins) by adding this line (search for it actions/checkout@v2 in github)
        uses: actions/checkout@v2 
        # with the help of these "actions" we do not need to have the hassle of what we need to explicityly perform using scripts(like in jenkins) we can directly search and make use of what actions are needed as per our required steps

        # now we need to configure the packer as we want to use the golden image  
        name: Setup the packer
        id: setup
        uses: hashicorp/setup-packer@main


        # initialise the packer 
        name: initilaise packer
        id: init packer
        run: "packer init ./image-builder/"

        # format packer
        name: format packer
        id: fmt packer
        run: "packer fmt ./image-builder/"

        # validate packer
        name: Validate packer image
        id: validate packer
        run: "packer validate ./image-builder/"

        # build packer image
        name: Build packer image
        id: build packer image
        run: "packer build ./image-builder/"

        # after packer we need to initialise the terraform for provisioning of the infrastructure using it

        # setup terraform
        name: setup terraform
        id: Setup Terraform
        uses: hashicorp/setup-terraform@v2

        # terraform initilaise
        name: terraform init
        id: initilaiseterraform
        run: terraform init
        
        # format terraform
        name: fomrat terraform
        id: format terraform
        run: terraform fmt

        # validate terraform
        name: terraform validate
        id: validateterraform
        run: terraform validate

        #plan terraform resources
        name: terraform plan
        id: planterraform
        # The -input=false indicates that we do not need input variables as to avoid the locking of state file and hasssle of creation of bucket for the other file.
        run: terraform plan -input=false -lock=false

        # apply terrafrom resources
        name: apply terraform 
        id: terraformapply
        run: terraform apply --auto-approve -lock=false
          

-------------------

While running the job it gives us the following error in packer build

Error: Datasource.Execute failed: no valid credential sources for  found.

Please see 
for more information about providing credentials.

Error: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors


  on image-builder/data.pkr.hcl line 1:
  (source code not available)


Error: Process completed with exit code 1.

NOTE: For the role_arn we have the credentails locally configured but for our workflow it dosent know how to add the credentails so as we have not passed our credentials in this case so we should add our credentails ie by creating it on the git using the "secret manager"


github repo => Settings ==> secret and variables (Actions) => create secret => and add the aws credentails for the access key and secret key
Push this changes to rerun the workflow


Also need to add those keys as the Environment variables in the provision.yml to establish a conection
Add the under the runs-on: ubuntu-latest (Runner as)
env: 
  AWS_ACCESS_KEY_ID: ${{ secrets.aws_access_key }} ==> It represents the secret named AWS_ACCESS_KEY taht we have added fore the repo
  AWS_SECRET_ACCESS_KEY: ${{ secrets.aws_secret_key }}  ==> represents the secret named AWS_SECRET_ACCESS_KEY that we have added for the repo

  We are setting up these values in the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY


----------------------------------

Now it will again fail on the terraform validate step gives error as 

Wed, 26 Jul 2023 10:42:09 GMT
Run terraform validate
Wed, 26 Jul 2023 10:42:09 GMT
/home/runner/work/_temp/91a86c06-0bf7-41d3-8ec7-354489449c6a/terraform-bin validate
Wed, 26 Jul 2023 10:42:12 GMT
╷
Wed, 26 Jul 2023 10:42:12 GMT
│ Error: Invalid function argument
Wed, 26 Jul 2023 10:42:12 GMT
│ 
Wed, 26 Jul 2023 10:42:12 GMT
│   on null_resource.tf line 12, in resource "null_resource" "provisioner":
Wed, 26 Jul 2023 10:42:12 GMT
│   12:     private_key = file("${path.module}/../keypair")
Wed, 26 Jul 2023 10:42:12 GMT
│     ├────────────────
Wed, 26 Jul 2023 10:42:12 GMT
│     │ while calling file(path)
Wed, 26 Jul 2023 10:42:12 GMT
│     │ path.module is "."
Wed, 26 Jul 2023 10:42:12 GMT
│ 
Wed, 26 Jul 2023 10:42:12 GMT
│ Invalid value for "path" parameter: no file exists at "./../keypair"; this
Wed, 26 Jul 2023 10:42:12 GMT
│ function works only with files that are distributed as part of the
Wed, 26 Jul 2023 10:42:12 GMT
│ configuration source code, so if this file will be created by a resource in
Wed, 26 Jul 2023 10:42:12 GMT
│ this configuration you must instead obtain this result from an attribute of
Wed, 26 Jul 2023 10:42:12 GMT
│ that resource.
Wed, 26 Jul 2023 10:42:12 GMT
╵
Wed, 26 Jul 2023 10:42:12 GMT
Error: Terraform exited with code 1.
Wed, 26 Jul 2023 10:42:12 GMT
Error: Process completed with exit code 1.


NOTE: Here we are adding our private key directly which should not be added so how can we pass the secret key so that should be used by our github workflow 

As we have done above we could just add the secret key for the repo and use it there 

Add it as SSH_PRIVATE_KEY and make reference of it in the provision.yml file in the env 

  provisioner "file" {
    # source      = "~/Downloads/new-ultimate-cicd.pem" # where do you want to fetch the file from
    destination = "/home/ec2-user/mykey" # to what destination we want the file to be taken to
    content     = file("${path.module}/../keypair")
  }

But also we need to pass it to the file proviosioner in the content field => so we get it as follows

WE need to make the change as follows
content = file("${path.module}/../keypair") ==> content = var.mykey

WE will pass here mykey as an environment variable to the variable.tf file

In varaible.tf ==> make a new entry for the variable mykey

variable "mykey" {
  type: string
  value: <We will not pass here>  ==> But while terraform plan it will ask for the "Enter the value" ==> WE will provide it via the Environment ENV_VARIABLES
}

Then add the environment variable for the secret key we added into the secrets of the repository
env:
  TF_VAR_mykey : ${{secrets.ssh_private_key}}

  Here by the TF_VAR_mykey it will consider the value as Environemnt variables
  ${{secrets.ssh_private_key}} => It will take the private key in the ssh_private_key which we have in which we added our private key



-----------
NOTE: We have the options to update the secrets but cannot see the existing secrets

------------------------

Steps to follow
scp -i keypair keypair ec2-user@3.236.165.134:~
ssh -i keypair ec2-user@10.0.0.
curl localhost


########################################  END Day 21 ########################################


########################################  Day 22 modules in tf########################################

Why do we need modules ?

To reuse our code, to make the changes in the same code and do not have to write greater length codebase
To follow dry principle, write once use multiple times here => Thats why we have wrutten dynamic code

Q@. how to use modules ==> 

Conidwer for example then remove the below code
make a folder named test ==> in that add a file main.tf

module "main" {
  source = ".."  ==> As we want to use our code as a module we need to get the path of all those files
  cidr_for_vpc = "10.0.0.0/24" => Also we need to pass all the variables in here that we were passing via variables, env variables

  Why so ? ==. so taht we can have the ability to run our module with different inputs as per the requiremnet making the code more reusable. 
}

--------------------

we do not have to write everything from the screatch we can make ue of the existing modules or we can leverage from here if it satisfies all our needs


Always meke the use of the reliable resources (genuine vendors) in the aws modules or rather any other resources

----------------------------

We will always try to make use of the reliable sources for the modules

Search for the autoscaling group modules in terraform ==> Set the version to 1.0.4 also check for dropdown contains the examples as asg_ec2, asg_elb ==> copy the code fopr proviosn instyructions as below

1. create a file app_server.tf 

module "autoscaling" {
  source  = "terraform-aws-modules/autoscaling/aws"
  version = "1.0.4"
  # insert the 10 required variables here
}

We need to create asg_ec2 so copy the code from the below only

module "asg" {
  source = "terraform-aws-modules/autoscaling/aws"

  # Launch configuration
  lc_name = "example-lc"

  image_id        = "ami-ebd02392"
  instance_type   = "t2.micro"
  security_groups = ["sg-12345678"]

  ebs_block_device = [
    {
      device_name           = "/dev/xvdz"
      volume_type           = "gp2"
      volume_size           = "50"
      delete_on_termination = true
    },
  ]

  root_block_device = [
    {
      volume_size = "50"
      volume_type = "gp2"
    },
  ]

  # Auto scaling group
  asg_name                  = "example-asg"
  vpc_zone_identifier       = ["subnet-1235678", "subnet-87654321"]
  health_check_type         = "EC2"
  min_size                  = 0
  max_size                  = 1
  desired_capacity          = 1
  wait_for_capacity_timeout = 0

  tags = [
    {
      key                 = "Environment"
      value               = "dev"
      propagate_at_launch = true
    },
    {
      key                 = "Project"
      value               = "megasecret"
      propagate_at_launch = true
    },
  ]
}


and remove the above code

module "autoscaling" {
  source  = "terraform-aws-modules/autoscaling/aws"
  version = "1.0.4"
  # insert the 10 required variables here
}


--------------

Now try planning it out using terraform plan ==> error

NOTE: We need to initialise the terraform whenever we update the modules as tfstate file would be updated here. And it will download the module in the terraform => modules folder and that would be used (so that all the code would be available locally)

--------------------------------

Or we could use the default version provided which is about v6.1.0 and move forward with the project with the 

module "autoscaling" {
  source  = "terraform-aws-modules/autoscaling/aws"
  version = "6.10.0"
  # insert the 1 required variable here
}

code 

module "asg" {
  source  = "terraform-aws-modules/autoscaling/aws"
  version = "6.10.0"
  # Autoscaling group
  name = "application -asg"
  #add the instance name field
  instance_name = "application-server"

  min_size                  = 0  # what are the minimum number of instances needed
  max_size                  = 1  # what are the maximum number of instances needed
  desired_capacity          = 1
  wait_for_capacity_timeout = 0
  health_check_type         = "EC2"
  # vpc_zone_identifier       = ["subnet-1235678", "subnet-87654321"]
  # we need to replace it with the private subnet as we want to add the app_Server in private subnet from the webserver.tf
  vpc_zone_identifier  = element([for each_subnet in aws_subnet.private_subnet : each_subnet.id], 1)

  #initial_lifecycle_hooks block is not required so we can remove that as well 
   #instance_referesh block is not required so we can remove that as well  but can keep it
  instance_refresh = {
    strategy = "Rolling"
    preferences = {
      checkpoint_delay       = 600
      checkpoint_percentages = [35, 70, 100]
      instance_warmup        = 300
      min_healthy_percentage = 50
    }
    triggers = ["tag"]
  }

  # Launch template
  launch_template_name        = "application-lt"
  launch_template_description = "Launch template example for application server"
  update_default_version      = true

  #need to replace ami id by the ami we desired it to be (amazon linux)

  #image_id          = "ami-ebd02392"
  image_id   = "ami-0f34c5ae932e6f0e4"
  instance_type     = "t3.micro"
  ebs_optimized     = true
  enable_monitoring = true
  # create a security group it is not by default added in the template we copied but we need to add 
  security_group = []

  # IAM role & instance profile
  create_iam_instance_profile = true
  iam_role_name               = "example-asg"
  iam_role_path               = "/ec2/"
  iam_role_description        = "IAM role example"
  iam_role_tags = {
    CustomIamRole = "Yes"
  }
  iam_role_policies = {
    AmazonSSMManagedInstanceCore = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
  }

  block_device_mappings = [
    {
      # Root volume
      device_name = "/dev/xvda"
      no_device   = 0
      ebs = {
        delete_on_termination = true
        encrypted             = true
        volume_size           = 20
        volume_type           = "gp2"
      }
    }, {
      device_name = "/dev/sda1"
      no_device   = 1
      ebs = {
        delete_on_termination = true
        encrypted             = true
        volume_size           = 30
        volume_type           = "gp2"
      }
    }
  ]

  capacity_reservation_specification = {
    capacity_reservation_preference = "open"
  }

  cpu_options = {
    core_count       = 1
    threads_per_core = 1
  }

  credit_specification = {
    cpu_credits = "standard"
  }

  instance_market_options = {
    market_type = "spot"
    spot_options = {
      block_duration_minutes = 60
    }
  }

  metadata_options = {
    http_endpoint               = "enabled"
    http_tokens                 = "required"
    http_put_response_hop_limit = 32
  }

  network_interfaces = [
    {
      delete_on_termination = true
      description           = "eth0"
      device_index          = 0
      security_groups       = ["sg-12345678"]
    },
    {
      delete_on_termination = true
      description           = "eth1"
      device_index          = 1
      security_groups       = ["sg-12345678"]
    }
  ]

  placement = {
    availability_zone = "us-west-1b"
  }

  tag_specifications = [
    {
      resource_type = "instance"
      tags          = { WhatAmI = "Instance" }
    },
    {
      resource_type = "volume"
      tags          = { WhatAmI = "Volume" }
    },
    {
      resource_type = "spot-instances-request"
      tags          = { WhatAmI = "SpotInstanceRequest" }
    }
  ]

  tags = {
    Environment = "dev"
    Project     = "megasecret"
  }
}

We need to make effort to combine all the security group in single file so we need to create the sg.tf and all the security groups in it there

#copy the security_group from the webserver and bastion_host ; also for application_server make the copy of the webserver and make chanegs  as per the need



# create the variables for the application server inbound rules same as for the "inbound_rules_webserver"

Now we want the app server to be connected from the web server only so we need to maek certain changes

So instead of the using the cidr_block we can make use of the security_group_chaining


security_group_chaining: It is more secure and more robust using as the IP addresses could chaneg after rebooting the instances but not the security groups (it will not chaneg overtime and rebooting instances will have no effect on them)

So instead of allowing the ip adress of websrver we can just allow the security group of the web server.

so we will be replacing the cidr_blocks inplace of that use the security_groups in the application server 

#   cidr_blocks = [aws_vpc.this.cidr_block]
security_groups = [aws_security_group.web_server.id]

Also remember to add the following code into the app_server.tf in the module asg
under image_id 
security_groups = [aws_security_group.application_server.id]
----------------------------------

Check block_device_mapping in app_server.tf

edit volume_size = 20  => volume_size = 10 (No need)

Remove network_interfaces block as we have defined the security group already

NOTE: Network interfaces internally have the security groups; as we have directly adding our custom security_groups we don not need it here.

Also comment in the below code 

  tags = {
    Environment = "dev"
    # Project     = "megasecret"
  }

Also not required so remove it from the code

  # IAM role & instance profile
  create_iam_instance_profile = true
  iam_role_name               = "example-asg"
  iam_role_path               = "/ec2/"
  iam_role_description        = "IAM role example"
  iam_role_tags = {
    CustomIamRole = "Yes"
  }
  iam_role_policies = {
    AmazonSSMManagedInstanceCore = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
  }


So the final code for the app_server.tf will look like as below

module "asg" {
  source  = "terraform-aws-modules/autoscaling/aws"
  version = "6.10.0"
  # Autoscaling group
  name = "application -asg"
  #add the instance name field
  instance_name = "application-server"

  min_size                  = 0 # what are the minimum number of instances needed
  max_size                  = 1 # what are the maximum number of instances needed
  desired_capacity          = 1
  wait_for_capacity_timeout = 0
  health_check_type         = "EC2"
  # vpc_zone_identifier       = ["subnet-1235678", "subnet-87654321"]
  # we need to replace it with the private subnet as we want to add the app_Server in private subnet from the webserver.tf
  vpc_zone_identifier = element([for each_subnet in aws_subnet.private_subnet : each_subnet.id], 1)

  #initial_lifecycle_hooks block is not required so we can remove that as well 
  #instance_referesh block is not required so we can remove that as well  but can keep it
  instance_refresh = {
    strategy = "Rolling"
    preferences = {
      checkpoint_delay       = 600
      checkpoint_percentages = [35, 70, 100]
      instance_warmup        = 300
      min_healthy_percentage = 50
    }
    triggers = ["tag"]
  }

  # Launch template
  launch_template_name        = "application-lt"
  launch_template_description = "Launch template example for application server"
  update_default_version      = true

  #need to replace ami id by the ami we desired it to be (amazon linux)

  #image_id          = "ami-ebd02392"
  image_id          = "ami-0f34c5ae932e6f0e4"
  instance_type     = "t3.micro"
  ebs_optimized     = true
  enable_monitoring = true
  # create a security group it is not by default added in the template we copied but we need to add 
  #add the security group of the application_server here
  security_groups = [aws_security_group.application_server.id]

  block_device_mappings = [
    {
      # Root volume
      device_name = "/dev/xvda"
      no_device   = 0
      ebs = {
        delete_on_termination = true
        encrypted             = true
        volume_size           = 10
        volume_type           = "gp2"
      }
      }, {
      device_name = "/dev/sda1"
      no_device   = 1
      ebs = {
        delete_on_termination = true
        encrypted             = true
        volume_size           = 30
        volume_type           = "gp2"
      }
    }
  ]

  capacity_reservation_specification = {
    capacity_reservation_preference = "open"
  }

  cpu_options = {
    core_count       = 1
    threads_per_core = 1
  }

  credit_specification = {
    cpu_credits = "standard"
  }

  instance_market_options = {
    market_type = "spot"
    spot_options = {
      block_duration_minutes = 60
    }
  }

  metadata_options = {
    http_endpoint               = "enabled"
    http_tokens                 = "required"
    http_put_response_hop_limit = 32
  }

  placement = {
    availability_zone = "us-west-1b"
  }

  tag_specifications = [
    {
      resource_type = "instance"
      tags          = { WhatAmI = "Instance" }
    },
    {
      resource_type = "volume"
      tags          = { WhatAmI = "Volume" }
    },
    {
      resource_type = "spot-instances-request"
      tags          = { WhatAmI = "SpotInstanceRequest" }
    }
  ]

  tags = {
    Environment = "dev"
    # Project     = "megasecret"
  }
}

If pops an error due to something check for the 

vpc_zone_identifier = element([for each_subnet in aws_subnet.private_subnet : each_subnet.id], 1)

It expects a string so we need to enclose it in "[]"

vpc_zone_identifier = [element([for each_subnet in aws_subnet.private_subnet : each_subnet.id], 1)]

-----------------------------------------------------

Now try to add the "application load balancer"  ==> Search for it in the aws_lb terraform  => pasted the template code for the application load balancer 

Add it into a new file => alb.tf
  resource "aws_lb" "test" {
    name               = "test-lb-tf"
    internal           = false
    load_balancer_type = "application"
    security_groups    = [aws_security_group.lb_sg.id]
    subnets            = [for subnet in aws_subnet.public : subnet.id]

    enable_deletion_protection = true

    access_logs {
      bucket  = aws_s3_bucket.lb_logs.id
      prefix  = "test-lb"
      enabled = true
    }

    tags = {
      Environment = "production"
    }
  }


  modify it like below

  resource "aws_security_group" "lb_sg" {
  name        = "allow_lb"
  description = "Allow access to load balancers from the internet"
  vpc_id      = aws_vpc.this.id

  ingress {
    description = "Allow access to load balancers from the internet"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_access_to_lb_from_internet"
  }
}

Also REMOVE the code from the alb.tf

    enable_deletion_protection = true

    access_logs {
      bucket  = aws_s3_bucket.lb_logs.id
      prefix  = "test-lb"
      enabled = true
    }

    tags = {
      Environment = "production"
    }


So the overall changes made to the aws_lb will acts as like alb.tf file

  resource "aws_lb" "this" {
    name               = "three-tier-alb"
    #it will not be internal as its internet facing
    internal           = false 
    # why we need the load_balacer_type as application its beacuse it helps in serving the request http,https etc

    # Read OSI Model  ==> 7 layer [pdntspa] ==> application layer is 7th layer and our application works at the 7th layer(where it can serve the protocols like http,https) 
    # for network_lb it works at the network layer(layer 4) and supports transport layer, supports tcp like communication

    # in the network_lb so we cannot have the URL or PATH based routing so we need to make use of the Application Load balancer in our case
    load_balancer_type = "application"
    #Need to create the security group for the application load balancers also; so add a security_group for the load_balancers in sg.tf

    #copy same as that of the bastion host
    
    security_groups    = [aws_security_group.lb_sg.id]

    # in this for the subnets we require atleast 2 subnets; we can take the subnets from the bastion_host.tf file
    subnets            = [for each_subnet in aws_subnet.public_subnet : each_subnet.id]
  }


  Also we need the target group as the load balancer without target group is of no use

  got to the link specified : https://registry.terraform.io/providers/hashicorp/aws/2.58.0/docs/resources/lb_target_group

  copy the code of the target group from there 

  resource "aws_lb_target_group" "test" {
  name     = "tf-example-lb-tg"
  port     = 80
  protocol = "HTTP"
  vpc_id   = "${aws_vpc.main.id}"
}

modified it to the 

resource "aws_lb_target_group" "test" {
    # as our lb will be sending the traffic to our "webserver" which actually runs at/ served at  port 80 [Apache]
    name     = "three-tier-tg"
    port     = 80
    protocol = "HTTP"
    vpc_id   =  aws_vpc.this.id
}


# we need to assosiate the target group with the load balancers for that we have the aws_lb_target_group_attachment

#aws_lb_target_group_attachment ==> what it does ? ==> It is used to attach to the "instances" 

# Search for it aws_lb_target_group_attachment  ==> 

resource "aws_lb_target_group_attachment" "test" {
  target_group_arn = aws_lb_target_group.test.arn
  target_id        = aws_instance.test.id
  port             = 80
}

modified it to the 

resource "aws_lb_target_group_attachment" "this" {
  #arn referes to amazon resource number  
  target_group_arn = "${aws_lb_target_group.this.arn}"
  # we need to apss the target_id as webserver as we do need the load_balancer to communicate to the webserver
  target_id        = "${aws_instance.web.id}"
  #webserver (Apache runs on PORT 80 so kept as it is)
  port             = 80
}



# We also need to configure the listners on our load balancers

# What Listners does : It integrates the target with load balancers
# from the link copy template https://registry.terraform.io/providers/hashicorp/aws/2.49.0/docs/resources/lb_listener

resource "aws_lb_listener" "front_end" {
  load_balancer_arn = "${aws_lb.front_end.arn}"
  port              = "443"
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-2016-08"
  certificate_arn   = "arn:aws:iam::187416307283:server-certificate/test_cert_rab3wuqwgja25ct3n4jdj2tzu4"

  default_action {
    type             = "forward"
    target_group_arn = "${aws_lb_target_group.front_end.arn}"
  }
}

modify it to the 

resource "aws_lb_listener" "this" {
  load_balancer_arn = aws_lb.this.arn
  port              = "80"
  protocol          = "HTTP"

  default_action {
    type             = "forward"
    #modify the target_group_arn to our aws_lb_target_group_attachment's arn
    target_group_arn = aws_lb_target_group.this.arn
  }
}


MODIFIED alb.tf file will look modified as below

resource "aws_lb" "this" {
  name = "three-tier-alb"
  #it will not be internal as its internet facing
  internal = false
  # why we need the load_balacer_type as application its beacuse it helps in serving the request http,https etc

  # Read OSI Model  ==> 7 layer [pdntspa] ==> application layer is 7th layer and our application works at the 7th layer(where it can serve the protocols like http,https) 
  # for network_lb it works at the network layer(layer 4) and supports transport layer, supports tcp like communication

  # in the network_lb so we cannot have the URL or PATH based routing so we need to make use of the Application Load balancer in our case
  load_balancer_type = "application"
  #Need to create the security group for the application load balancers also; so add a security_group for the load_balancers in sg.tf

  #copy same as that of the bastion host

  security_groups = [aws_security_group.lb_sg.id]

  # in this for the subnets we require atleast 2 subnets; we can take the subnets from the bastion_host.tf file
  subnets = [for each_subnet in aws_subnet.public_subnet : each_subnet.id]
}

resource "aws_lb_target_group" "this" {
  # as our lb will be sending the traffic to our "webserver" which actually runs at/ served at  port 80 [Apache]
  name     = "three-tier-tg"
  port     = 80
  protocol = "HTTP"
  vpc_id   = aws_vpc.this.id
}


# we need to assosiate the target group with the load balancers for that we have the aws_lb_target_group_attachment

#aws_lb_target_group_attachment ==> what it does ? ==> It is used to attach to the "instances" 

# Search for it aws_lb_target_group_attachment  ==> 

resource "aws_lb_target_group_attachment" "test" {
  #arn referes to amazon resource number  
  target_group_arn = aws_lb_target_group.this.arn
  # we need to apss the target_id as webserver as we do need the load_balancer to communicate to the webserver
  target_id = aws_instance.web.id
  #webserver (Apache runs on PORT 80 so kept as it is)
  port = 80
}


# We also need to configure the listners on our load balancers

# What Listners does : It integrates the target with load balancers
# from the link copy template https://registry.terraform.io/providers/hashicorp/aws/2.49.0/docs/resources/lb_listener


resource "aws_lb_listener" "this" {
  load_balancer_arn = aws_lb.this.arn
  port              = "80"
  protocol          = "HTTP"

  default_action {
    type = "forward"
    #modify the target_group_arn to our aws_lb_target_group_attachment's arn
    target_group_arn = aws_lb_target_group.this.arn
  }
}



MODIFIED app_server.tf will look  modified as

module "asg" {
  source  = "terraform-aws-modules/autoscaling/aws"
  version = "6.10.0"
  # Autoscaling group
  name = "application -asg"
  #add the instance name field
  instance_name = "application-server"

  min_size                  = 0 # what are the minimum number of instances needed
  max_size                  = 1 # what are the maximum number of instances needed
  desired_capacity          = 1
  wait_for_capacity_timeout = 0
  health_check_type         = "EC2"
  # vpc_zone_identifier       = ["subnet-1235678", "subnet-87654321"]
  # we need to replace it with the private subnet as we want to add the app_Server in private subnet from the webserver.tf
  vpc_zone_identifier = [element([for each_subnet in aws_subnet.private_subnet : each_subnet.id], 1)]

  #initial_lifecycle_hooks block is not required so we can remove that as well 
  #instance_referesh block is not required so we can remove that as well  but can keep it
  instance_refresh = {
    strategy = "Rolling"
    preferences = {
      checkpoint_delay       = 600
      checkpoint_percentages = [35, 70, 100]
      instance_warmup        = 300
      min_healthy_percentage = 50
    }
    triggers = ["tag"]
  }

  # Launch template
  launch_template_name        = "application-lt"
  launch_template_description = "Launch template example for application server"
  update_default_version      = true

  #need to replace ami id by the ami we desired it to be (amazon linux)

  #image_id          = "ami-ebd02392"
  image_id          = "ami-0f34c5ae932e6f0e4"
  instance_type     = "t3.micro"
  ebs_optimized     = true
  enable_monitoring = true
  # create a security group it is not by default added in the template we copied but we need to add 
  #add the security group of the application_server here
  security_groups = [aws_security_group.application_server.id]

  block_device_mappings = [
    {
      # Root volume
      device_name = "/dev/xvda"
      no_device   = 0
      ebs = {
        delete_on_termination = true
        encrypted             = true
        volume_size           = 10
        volume_type           = "gp2"
      }
      }, {
      device_name = "/dev/sda1"
      no_device   = 1
      ebs = {
        delete_on_termination = true
        encrypted             = true
        volume_size           = 30
        volume_type           = "gp2"
      }
    }
  ]

  capacity_reservation_specification = {
    capacity_reservation_preference = "open"
  }

  cpu_options = {
    core_count       = 1
    threads_per_core = 1
  }

  credit_specification = {
    cpu_credits = "standard"
  }

  instance_market_options = {
    market_type = "spot"
    spot_options = {
      block_duration_minutes = 60
    }
  }

  metadata_options = {
    http_endpoint               = "enabled"
    http_tokens                 = "required"
    http_put_response_hop_limit = 32
  }

  placement = {
    availability_zone = "us-west-1b"
  }

  tag_specifications = [
    {
      resource_type = "instance"
      tags          = { WhatAmI = "Instance" }
    },
    {
      resource_type = "volume"
      tags          = { WhatAmI = "Volume" }
    },
    {
      resource_type = "spot-instances-request"
      tags          = { WhatAmI = "SpotInstanceRequest" }
    }
  ]

  tags = {
    Environment = "dev"
    # Project     = "megasecret"
  }
}

MODIFIED sg.tf modified will look as 

# webserver sg
resource "aws_security_group" "web_server" {
  name        = "allow_webserver"
  description = "Allow web traffic"
  vpc_id      = aws_vpc.this.id

  dynamic "ingress" {
    for_each = var.inbound_rules_web
    content {
      description = ingress.value.description
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = ingress.value.protocol
      cidr_blocks = [aws_vpc.this.cidr_block]
    }
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_webserver"
  }
}

# appplication server sg
resource "aws_security_group" "application_server" {
  name        = "allow_application_traffic"
  description = "Allow application traffic"
  vpc_id      = aws_vpc.this.id
  dynamic "ingress" {
    for_each = var.inbound_rules_application
    content {
      description = ingress.value.description
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = ingress.value.protocol
      #   cidr_blocks = [aws_vpc.this.cidr_block]
      security_groups = [aws_security_group.web_server.id]
    }
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_webserver"
  }
}

#bastion host sg

resource "aws_security_group" "bastion_host" {
  name        = "allow_bastion_host"
  description = "Allow ssh into the private subnet resources using this"
  vpc_id      = aws_vpc.this.id

  ingress {
    description = "Allow the ssh traffic to private subnet bastion host from private subnet"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_bastion_host_security"
  }
}

# application_lb sg

resource "aws_security_group" "lb_sg" {
  name        = "allow_lb"
  description = "Allow access to load balancers from the internet"
  vpc_id      = aws_vpc.this.id

  ingress {
    description = "Allow access to load balancers from the internet"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_access_to_lb_from_internet"
  }
}

MODIFIED variable.tf will be [adding this up in the file]

#add the variable for inbound_rules_application(Application server)

variable "inbound_rules_application" {
  description = "ingress rules for security group of application server"
  type = list(object({
    port        = number
    description = string
    protocol    = string
  }))

  #As it would be running the Tomcat Application we will be needing port 8080 in inbound rules and we would be removing ssh [port 22] for that we will add manually its one of the usecase
  default = [{
    port        = 8080
    description = "This is for the application hosting"
    protocol    = "tcp"
  }]
}

Summary of Day 22

We have moved the security_groups from the webserver.tf and the bastion_host.tf to the sg.tf
asg was introduced using the modules in terraform
added the application load balancers, target_group, target_group_attachement, aws_lb_listeners


########################################  END Day 22 ########################################

######################################## Day 23 #############################################

We are going to implement the RDS in the aws resources

Arooura RDS ==> Completely AWS based service, It acts a s a wrappaer to the mysql+ posgresql for performance enhacement

Ways to install database in AWS ==>

1. launch an ec2 instance ==> configure everything from scratch ==> need to take care of everything patching,managing, backup, maintainance

2. By using AWS RDS => we can do all of the above

--------------------------------------------------


Now lets start applying RDS in our solution  => create rds.tf       

use template from here (basic usage template) [NOTE: Never recommended to go for latest version ==> v5.6.1]
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/db_instance


resource "aws_db_instance" "default" {
  allocated_storage    = 10
  db_name              = "mydb"
  engine               = "mysql"
  engine_version       = "5.7"
  instance_class       = "db.t3.micro"
  username             = "foo"
  password             = "foobarbaz"
  parameter_group_name = "default.mysql5.7"
  skip_final_snapshot  = true
}

modified to the 

resource "aws_db_instance" "default" {
  allocated_storage    = 10
  db_name              = "threetierdb"
  engine               = "mysql"
  # need to verify ift with the latest engine version via going to aws console for checking engine version ==> aws rds
  engine_version       = "8.0.32"
  instance_class       = "db.t3.micro"
  # as its sensitive information for both username and password
  username             = var.db_user_name
  password             = var.db_password
  #remove the parameter_group_name field for now
  #   parameter_group_name = "default.mysql5.7"
  skip_final_snapshot  = true

  #Add security groups, so we firstly need to create the security group for this
  security_groups = [aws_security_group.db_sg.id]  
}


also the security_group is now added so we get it as we add the following code in it

# rds sg
resource "aws_security_group" "db_sg" {
  name        = "allow_db"
  #As we know only our application server should connect to the database [rds] 
  description = "Allow db access from the application server"
  vpc_id      = aws_vpc.this.id

  ingress {
    description = "Allow db access from the application server"
    #for mysql default port is 3306 so need to change it to the 3306
    from_port   = 3306
    to_port     = 3306
    protocol    = "tcp"
    # instead of cidr block allow to access from the vpc_security_group_id of the application_server
    # cidr_blocks = ["0.0.0.0/0"]
    vpc_security_group_ids = [aws_security_group.db_sg.id]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "allow_db_access"
  }
}

Add the varaibls that are made in the variables.tf

variable "db_user_name" {
  description = "Username to connect with db"
  type = string
  #to hide the sensitive info
  sensitive = true
}

variable "db_password" {
  description = "Password to connect with db"
  type = string
  #to hide the sensitive info
  sensitive = true
}


-----------------------------------------
Along with that we need to add the "subnet_group" 

AS we do not have one so we need to create the subnet groups

Copy the template code from the https://registry.terraform.io/providers/-/aws/5.1.0/docs/resources/db_subnet_group

resource "aws_db_subnet_group" "default" {
  name       = "main"
  subnet_ids = [aws_subnet.frontend.id, aws_subnet.backend.id]

  tags = {
    Name = "My DB subnet group"
  }
}

modify it to the 

resource "aws_db_subnet_group" "this" {
  name       = "threetierdb_subnet_group"
  #subnets should lie into the private subnets so we need to add private subnets from the app_server.tf
  subnet_ids = [for each_subnet in aws_subnet.private_subnet : each_subnet.id]

  tags = {
    Name = "threetier_db_subnet"
  }
}


Then what we need to do is we need to pass the variables to the proviosn.yml file to ENV Variables and run 
the github actions workflow smoothly

Add the env variables like ==>
  TF_VAR_db_user_name: ${{ secrets.db_user_name }}
  TF_VAR_db_password: ${{ secrets.db_password }}

Add the secrets for the db in the Github Repo with the secrets

DB_USER_NAME = admin
DB_PASSWORD = admin123


Now run the pipeline/ github actions here like by commiting the code ==> 

git commit -am "code added for rds, chnagesin provison.yml"

git push

All should work here
---------------------------------------

Here we will segregate the packer build and the terraform build seperately 

build.yml ==> Will contain all the content of the packer
provison.yml ==> Will contain all the content of the Terraform



Modified build.yml will look like 

on:  
    workflow_dispatch: 
    # push:
    # # on the push on the patghs below we shuld trigger the build.yml pipeline 
    #     paths:
    #         - .github/workflows/build.yml
    #         - ../image-builder
    #     branches: [main]    
jobs: 
    provison-three-tier: 
        runs-on: ubuntu-latest
        env:
            AWS_ACCESS_KEY_ID: ${{ secrets.aws_access_key }}
            AWS_SECRET_ACCESS_KEY: ${{ secrets.aws_secret_key }}
        steps:
            -   name: checkout
                uses: actions/checkout@v2
        
            -   name: Setup the packer
                id: setup
                uses: hashicorp/setup-packer@main

            -   name: initialise packer
                id: init-packer
                run: "packer init ./image-builder/"

            -   name: format packer
                id: fmt-packer
                run: "packer fmt ./image-builder/"

            -   name: Validate packer image
                id: validate-packer
                run: "packer validate ./image-builder/"

            -   name: Build packer image
                id: build-packer-image
                run: "packer build ./image-builder/"


The modified provison.yml will look like 

name: Provisioning of threee tier architecture using Terraform
on: 
  workflow_dispatch: 
  # push:
  #   branches: [main]

jobs: 
  provison-three-tier: 
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.aws_access_key }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.aws_secret_key }}
      TF_VAR_mykey : ${{secrets.ssh_private_key}}
      TF_VAR_db_user_name: ${{ secrets.db_user_name }}
      TF_VAR_db_password: ${{ secrets.db_password }}
      
    steps:
      - name: checkout
        uses: actions/checkout@v2 

      - name: setup terraform
        id: setup-tf
        uses: hashicorp/setup-terraform@v2

      - name: terraform init
        id: init-tf
        run: terraform init
        
      - name: format terraform
        id: fmt-tf
        run: terraform fmt

      - name: terraform validate
        id: valid-tf
        run: terraform validate

      - name: terraform plan
        id: plan-tf
        run: terraform plan -input=false -lock=false

      - name: apply terraform 
        id: apply-tf
        run: terraform apply --auto-approve -lock=false

We are here commenting the push beacuse we want manual dispatch for now so we get as follows        

-------------------

creating a seperate branch for three-tier-application ==> Then try tio merge to the main branch
######################################## END Day 23 #########################################

######################################## Day 24 #########################################


Checkout the infra created in Day 23 by checking all the services used they are the EC2 Instances, VPC, AMI's , Laod balancers,

For checking out the load balancers we need to use the following like DNS names
DNS NAMES: three-tier-alb-444622722.us-east-1.elb.amazonaws.com

It should give us the apache script that we have written in them


Tho check the auto-scaling group ==> goto the EC2 instances ==> Side navigations bottomest field refers to Autoscaling

our application server we have builted it by using the autoscaling group so if anything arises we can see its Activity.
--------------------------
Also cross-verify check for the security groups wich are attached to the web server and the application server also the bastion host
-----------------------------

While checking we found that the in the webserver instances security group we are allowing port 22 from the load balancers but it is not possible as we do NOT have te access via load balncers --> webserver

the correct path is : load  server --> bastion_host --> we_server

So we need to modify the security group for ssh(22) to the bastion_host's security group. So the existing group should be replaced by this sg-0004483d0be15cd64 / allow_bastion_host

----------------
Try ssh into the astion then weberver and then appolication server to cross-verify if everything is working or not

ssh -i keypair ec2-user@<publc-ip-address>

ssh -i mykey ec2-user@<private-ip-address>
------------------------------
If arisd this error naemd ==> Load key "mykey"  error in libcrypto

It is due to the the file provisoner that we have used in the null_resource.tf because it is corrupting the key and sends it to the desired location.
That is why do not recommend to use the provisioners

better way is to use the scp commands

we should not have the ssh key in the bastion host
-----------------------------------------

For adding the database that should only be accessibkle by the appplication server we created a "rds.tf" file

resource "aws_db_instance" "default" {
  allocated_storage      = 10
  db_name                = "threetierdb"
  engine                 = "mysql"
  engine_version         = "8.0.32"
  instance_class         = "db.t3.micro"
  username               = var.db_user_name
  password               = var.db_password
  skip_final_snapshot    = true
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  db_subnet_group_name   = aws_db_subnet_group.this.id
}

resource "aws_db_subnet_group" "this" {
  name = "threetierdb_subnet_group"
  #subnets should lie into the private subnets so we need to add private subnets from the app_server.tf
  subnet_ids = [for each_subnet in aws_subnet.private_subnet : each_subnet.id]

  tags = {
    Name = "threetier_db_subnet"
  }
}

We caan check the load balancers having the listeners roles configiured

If there any one request on PORT 80 then it redirects it to the three-tier-tg (our created target group created by terraform configuration ==. which is targetting ourt webserver [which is alos created by terraform code])

Once we reach the rds then install the mysql server by the following steps like as follows

# mysql install 

sudo rpm --import https://repo.mysql.com/RPM-GPG-KEY-mysql-2022
wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm
sudo yum localinstall -y mysql57-community-release-el7-8.noarch.rpm
sudo yum install -y mysql-community-server

sudo systemctl start mysqld 
sudo systemctl enable mysqld 

Now we need to provide the endpoint in here so we can do it with the help of rds


RDS ==> Databases in Sidenav ==> Terrraform file [gnereated by script] ==> Endpoint

terraform-20230822131216976300000007.cjsu2evn5dzp.us-east-1.rds.amazonaws.com
terraform-20230823042029713400000007.cjsu2evn5dzp.us-east-1.rds.amazonaws.com

add the command like 

mysql -h terraform-20230823042029713400000007.cjsu2evn5dzp.us-east-1.rds.amazonaws.com -u admin -p

It will ask for the password here [admin123]

show databases;
use threetierdb;
show tables;

exit;
-------------------------------------

Now we need to install the tomcat server on the Application Server here

so just do the following steps like

#Install 
sudo yum install java -y

# Install Tomcat
sudo su 
cd /
cd opt

wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.79/bin/apache-tomcat-9.0.79.tar.gz 

#unzip tomcat binary
tar -zvxf apache-tomcat-9.0.79.tar.gz

cd apache-tomcat-folder

cd bin

chmod +x startup.sh
chmod +x shutdown.sh


Create link files for Tomcat Server up and Down

ln -s /opt/apache-tomcat-9.0.79/bin/startup.sh /usr/local/bin/tomcatup

ln -s /opt/apache-tomcat-9.0.79/bin/shutdown.sh /usr/local/bin/tomcatdown

tomcatup

curl localhost:8080 [It should work]

So for now it will work from our localhost to connect; but if we are accessing it from the othr locations the this could cause the trouble for us. so we need to make the configuration changes

cd /opt/apache-tomcat-9.0.79
find -name context.xml

Then go to the file vim ./webapps/docs/META-INF/context.xml ==> comment the value tag using the HTML Comments

NOTE: shift + G makes your cusror to go to the last line

Then go to the file vim ./webapps/examples/META-INF/context.xml ==> comment the value tag using the HTML Comments

Then go to the file  vim ./webapps/host-manager/META-INF/context.xml ==> comment the value tag using the HTML Comments

Then go to the file vim ./webapps/manager/META-INF/context.xml ==> comment the value tag using the HTML Comments

./conf/context.xml
/webapps/docs/META-INF/context.xml
./webapps/examples/META-INF/context.xml
./webapps/host-manager/META-INF/context.xml
./webapps/manager/META-INF/context.xml


#comment value tag sections in below all files
vi ./webapps/examples/META-INF/context.xml
vi ./webapps/host-manager/META-INF/context.xml
vi ./webapps/manager/META-INF/context.xml




Update user information in tomcat-users.xml
cd apache-tomcat-9.0.76
cd conf
vi tomcat-users.xml

#Add below lines between <tomcat-users> tag

<role rolename="manager-gui"/>
<role rolename="manager-script"/>
<role rolename="manager-jmx"/>
<role rolename="manager-status"/>   
<user username="admin" password="admin" roles="manager-
gui,manager-script,manager-jmx,manager-status, “admin-gui"/>
<user username="deployer" password="deployer" roles="manager-script"/>
<user username="tomcat" password="s3cret" roles="manager-gui"/>

----------------------

But how would we access our application servr in the browser then ?

test by curl localhost:8080

We need to go back to our web servers ==> make some settings for the proxies

exit from the rds
exit from the application server
cd ~
cd /etc/httpd
cd conf.d/
create a new file named ==> sudo vim proxypass.conf

sudo vi proxy pass.conf

Then add the below text into the ProxyPass.conf file

ProxyPass /home http://10.0.0.52:8080  [http://<private-ip-of-application-server>:8080]
ProxyPassReverse /home http://10.0.0.52:8080 [http://<private-ip-of-application-server>:8080]

sudo service httpd restart

Now to verify our changes go to the load balancers then add the ==> Copy the DNS NAME ==> http://three-tier-alb-1940867002.us-east-1.elb.amazonaws.com/


And now try rto add the endpoint /home what we have added into the ProxyPass.conf file

http://three-tier-alb-1940867002.us-east-1.elb.amazonaws.com/home

The message would appear on the screen as [ If you're seeing this, you've successfully installed Tomcat. Congratulations! ]

What if we click on the manager Appplcation How to ?
It will show URL not found so we need to add them explictly into the proxypass.conf file here.

So the changes needed toi be made are as as follows

ProxyPass /manager http://10.0.0.52:8080/manager
ProxyPassReverse /manager http://10.0.0.52:8080/manager

again restart the httpd server ==> sudo service httpd restart

It would ask for the username and the password here ==> so here we get it as in the tomcat-users.xml file

username= admin
password = admin

Same for the Hostmanager also 

ProxyPass /host-manager http://10.0.0.52:8080/host-manager
ProxyPassReverse /host-manager http://10.0.0.52:8080/host-manager

Giving the forbidden error we need to resolve it by adding the admin-gui  present in the roles of the tomcat-users.xml in the application server only. 

cd /opt/apache-tomcat-9.0.79
cd conf
vi tomcat-users.xml

make a change in the line below just removing the starting quote of admin-gui

<user username="admin" password="admin" roles="manager-
gui,manager-script,manager-jmx,manager-status,admin-gui"/>



-------------------------------------------------

But for automating it it will surely cause lot of time to pass the Private Ip adress of Application server and IP will be changing constantly in it after each rerun of terraform code. [In the Proxypass.conf]

There may be some other reason also as it has the auto-scaling group here so IP can also change due to that; so this must be automated in the 1st case

So for that what we could add is to add the loadbalancer in between the webserver and appserver [task to do]

But we can manage it via the Ansible alos here.

For Day 25 we will be considering the Integration test on Github Actions [packer and terraform] using the Terratest
######################################## END Day 24 #########################################